{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III - Uncertainty\n",
    "## Project 3a - PageRank\n",
    "\n",
    "[Course Link](https://cs50.harvard.edu/ai/)\n",
    "\n",
    "[Project Instructions](https://cs50.harvard.edu/ai/projects/2/pagerank/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PageRank\n",
    "PageRank is an algrorithm created by by Google’s co-founders that determines the importance of a website for search result purposes. In PageRank’s algorithm, a website is more important if it is linked to by other important websites, and links from less important websites have their links weighted less. \n",
    "\n",
    "In this project, you’ll calculae a website PageRank in two ways, one by sampling pages from a Markov Chain Random Surfer and another by iteratively applying the PageRank formula.\n",
    "\n",
    "\n",
    "**Note, the code to run the project is at the bottom of the file. If you execute all cells in the notebook it should work.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, global vars, and functions used for all solutions\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DAMPING = 0.85\n",
    "SAMPLES = 10_000\n",
    "\n",
    "def crawl(directory):\n",
    "    \"\"\"\n",
    "    Parse a directory of HTML pages and check for links to other pages.\n",
    "    Return a dictionary where each key is a page, and values are\n",
    "    a list of all other pages in the corpus that are linked to by \n",
    "    the page.\n",
    "    \"\"\"\n",
    "    pages = dict()\n",
    "\n",
    "    # Extract all links from HTML files\n",
    "    for filename in os.listdir(directory):\n",
    "        if not filename.endswith(\".html\"):\n",
    "            continue\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents = f.read()\n",
    "            links = re.findall(r\"<a\\s+(?:[^>]*?)href=\\\"([^\\\"]*)\\\"\", contents)\n",
    "            pages[filename] = set(links) - {filename}\n",
    "\n",
    "    # Only include links to other pages in the corpus\n",
    "    for filename in pages:\n",
    "        pages[filename] = set(\n",
    "            link for link in pages[filename]\n",
    "            if link in pages\n",
    "        )\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Markov Transition Model\n",
    "\n",
    "This function accepts three arguments: corpus, page, and damping_factor.\n",
    "\n",
    "* **corpus** is a Python dictionary mapping a page name to a set of all pages linked to by that page.\n",
    " \n",
    " \n",
    "* **page** is a string representing which page the random surfer is currently on.\n",
    "\n",
    "    \n",
    "* **damping_factor** is a floating point number representing the damping factor to be used when generating the probabilities.\n",
    "\n",
    "The return value of the function should be a Python dictionary with one key for each page in the corpus. Each key should be mapped to a value representing the probability that a random surfer would choose that page next. The values in this returned probability distribution should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_model(directory, damping_factor, page):\n",
    "    corpus = crawl(directory)  \n",
    "    page_links = corpus[page]\n",
    "  \n",
    "    if len(page_links) != 0:\n",
    "        page_probs = {}\n",
    "        sumcheck = 0\n",
    "        rand_page_prob = (1 - damping_factor) / len(corpus)\n",
    "        rand_link_prob = damping_factor / len(page_links) + rand_page_prob  \n",
    "        \n",
    "        for page in corpus:\n",
    "            if page in page_links:\n",
    "                page_probs.update({page : rand_link_prob})\n",
    "                sumcheck += page_probs[page]\n",
    "            else:\n",
    "                page_probs.update({page : rand_page_prob})\n",
    "                sumcheck += page_probs[page]\n",
    "        #print(f'{sumcheck:.4f}  <------should be 1, if not, error in code.')\n",
    "        return page_probs   \n",
    "    else:\n",
    "        no_link_probs = {p : 1 / len(corpus) for p in corpus}\n",
    "        return no_link_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4.html': 0.037500000000000006,\n",
       " '1.html': 0.4625,\n",
       " '2.html': 0.037500000000000006,\n",
       " '3.html': 0.4625}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_model('corpus0', DAMPING, '2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'search.html': 0.021428571428571432,\n",
       " 'bfs.html': 0.021428571428571432,\n",
       " 'dfs.html': 0.021428571428571432,\n",
       " 'minimax.html': 0.4464285714285714,\n",
       " 'tictactoe.html': 0.021428571428571432,\n",
       " 'minesweeper.html': 0.021428571428571432,\n",
       " 'games.html': 0.4464285714285714}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_model('corpus1', DAMPING, 'tictactoe.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai.html': 0.125,\n",
       " 'c.html': 0.125,\n",
       " 'logic.html': 0.125,\n",
       " 'algorithms.html': 0.125,\n",
       " 'inference.html': 0.125,\n",
       " 'python.html': 0.125,\n",
       " 'recursion.html': 0.125,\n",
       " 'programming.html': 0.125}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_model('corpus2', DAMPING, 'recursion.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Random Surfer Instructions\n",
    "One way to think about PageRank is with the random surfer model, which considers the behavior of a hypothetical surfer on the internet who clicks on links at random.\n",
    "\n",
    "**One way to interpret this model is as a Markov Chain, where each page represents a state, and each page has a transition model that chooses among its links at random.** At each time step, the state switches to one of the pages linked to by the current state.\n",
    "\n",
    "**By sampling states randomly from the Markov Chain, we can get an estimate for each page’s PageRank. We can start by choosing a page at random, then keep following links at random, keeping track of how many times we’ve visited each page. After we’ve gathered all of our samples (based on a number we choose in advance), the proportion of the time we were on each page might be an estimate for that page’s rank.**\n",
    "\n",
    "To ensure we can always get to somewhere else in the corpus of web pages, we’ll introduce to our model a damping factor d. With probability d (where d is usually set around 0.85), **the random surfer will choose from one of the links on the current page at random. But otherwise (with probability 1 - d), the random surfer chooses one out of all of the pages in the corpus at random (including the one they are currently on).**\n",
    "\n",
    "Our random surfer now starts by choosing a page at random, and then, for each additional sample we’d like to generate, chooses a link from the current page at random with probability d, and chooses any page at random with probability 1 - d. If we keep track of how many times each page has shown up as a sample, we can treat the proportion of states that were on a given page as its PageRank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Surfur Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_sample(directory, corpus, damping_factor, prior_sample):\n",
    "    if prior_sample == None:  \n",
    "        # Start with random page\n",
    "        initial_state = random.choice(list(corpus.keys()))      \n",
    "        return initial_state\n",
    "    else: \n",
    "        # Use tranistion model probabilities for random sample\n",
    "        tm = transition_model(directory, damping_factor, prior_sample)\n",
    "        pages = list(corpus.keys())\n",
    "        probs = [value for key, value in tm.items()]\n",
    "        sample = np.random.choice(pages, 1, p=probs).item()\n",
    "        return sample\n",
    "\n",
    "def sample_pagerank(directory, damping_factor, n):\n",
    "    corpus = crawl(directory)\n",
    "    pageranks = {}\n",
    "    m_chain = []\n",
    "    \n",
    "    # Create Markov Chain of Pages from Random Samples\n",
    "    for i in range(0, SAMPLES):\n",
    "        if i == 0:\n",
    "            sample = get_rand_sample(directory, corpus, damping_factor, None)\n",
    "            m_chain.append(sample)\n",
    "        else:\n",
    "            sample = get_rand_sample(directory,corpus, damping_factor, m_chain[-1])\n",
    "            m_chain.append(sample)\n",
    "    \n",
    "    page_counts = pd.Series(m_chain).value_counts().to_dict()\n",
    "    page_ranks = {x:v/SAMPLES for x, v in page_counts.items()}\n",
    "\n",
    "    return(page_ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Algorithm Instructions\n",
    "\n",
    "We can also define a page’s PageRank using a recursive mathematical expression. \n",
    "\n",
    "In this model we let:\n",
    "* PR(p) = the PageRank of a given page p (the probabilty that a random surfer ends up on that page)\n",
    "\n",
    "There are two ways a random surfer could end up on the page:\n",
    "\n",
    "     1. With probability 1 - d, the surfer chose a page at random and ended up on page p\n",
    "\n",
    "     2. With probabilty d, the surfer followed a link from page i to page p\n",
    "     \n",
    "     \n",
    "The first condition is fairly straightforward to express mathematically: it’s 1 - d divided by N, where N is the total number of pages across the entire corpus. This is because the 1 - d probability of choosing a page at random is split evenly among all N possible pages.\n",
    "\n",
    "For the second condition, we need to consider each possible page i that links to page p. For each of those incoming pages, let NumLinks(i) be the number of links on page i. Each page i that links to p has its own PageRank, PR(i), representing the probability that we are on page i at any given time. And since from page i we travel to any of that page’s links with equal probability, we divide PR(i) by the number of links NumLinks(i) to get the probability that we were on page i and chose the link to page p.\n",
    "\n",
    "Thus the definition for PR(p) is:\n",
    "\n",
    "## $ PR(p) = \\frac{1-d}{N} + d * \\sum_{i} \\frac{PR(i)}{NumLinks(i)} $\n",
    "\n",
    "In this formula, d is the damping factor, N is the total number of pages in the corpus, i ranges over all pages that link to page p, and NumLinks(i) is the number of links present on page i.\n",
    "\n",
    "How would we go about calculating PageRank values for each page, then? We can do so via iteration: start by assuming the PageRank of every page is 1 / N (i.e., equally likely to be on any page). Then, use the above formula to calculate new PageRank values for each page, based on the previous PageRank values. If we keep repeating this process, calculating a new set of PageRank values for each page based on the previous set of PageRank values, eventually the PageRank values will converge (i.e., not change by more than a small threshold with each iteration).\n",
    "\n",
    "### Iterative Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getranks(corpus, old_ranks, final_ranks):\n",
    "    o_ranks = old_ranks\n",
    "    f_ranks = final_ranks\n",
    "    new_ranks = {}\n",
    "      \n",
    "    for page, links in corpus.items():\n",
    "        new_rank = 0 \n",
    "        \n",
    "        # Calculate New Rank Using PageRank Formula:\n",
    "        for pg, link in links.items(): \n",
    "            if len(links) > 0:\n",
    "                new_rank += o_ranks[pg] / link\n",
    "        new_rank *= DAMPING\n",
    "        new_rank += (1 - DAMPING) / len(corpus)\n",
    "                \n",
    "        if abs(new_rank - o_ranks[page]) <= .0001:\n",
    "            f_ranks.update({page: new_rank})\n",
    "            new_ranks.update({page: new_rank})\n",
    "                \n",
    "        new_ranks.update({page: new_rank})\n",
    "                \n",
    "    if len(o_ranks) == len(f_ranks):\n",
    "        return(f_ranks)   \n",
    "    else:\n",
    "        o_ranks = new_ranks        \n",
    "        return getranks(corpus, o_ranks, f_ranks)\n",
    "    \n",
    "\n",
    "def pagerank2(directory, d):\n",
    "    corpus = crawl(directory)\n",
    "    new_corpus = {}\n",
    "    \n",
    "    # New corpus created with pages and incoming links to the page\n",
    "    # Note: old corpus has pages with outgoing links from the page\n",
    "    for page in corpus:\n",
    "        links = {}\n",
    "        for pg, lnk in corpus.items():\n",
    "            if page in lnk:\n",
    "                links.update({pg:len(lnk)})\n",
    "            else:\n",
    "                pass\n",
    "        new_corpus.update({page: links})\n",
    "    \n",
    "    # If a page in original corpus has no outgoing links, then\n",
    "    # per project instructions, this means it actually has an outgoing\n",
    "    # link to every page in the corpus.\n",
    "    # So, when a page with no links is found, I add the page from\n",
    "    # the old corpus with no outgoing links to every link inside of \n",
    "    # new_corpus to reflect this. \n",
    "    for page, links in corpus.items():\n",
    "        if len(links) == 0:\n",
    "            pg = page\n",
    "            for p, l in new_corpus.items():\n",
    "                l.update({pg : len(new_corpus)})\n",
    "\n",
    "    old_ranks = {page: 1 / len(new_corpus) for page in new_corpus}\n",
    "    final_ranks = getranks(new_corpus, old_ranks, final_ranks={})\n",
    "    return(final_ranks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4.html': 0.13099022817761485,\n",
       " '1.html': 0.21988476390560002,\n",
       " '2.html': 0.4292402440111851,\n",
       " '3.html': 0.21988476390560002}"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pagerank2('corpus0', DAMPING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(directory):\n",
    "    ranks = sample_pagerank(directory, DAMPING, SAMPLES)\n",
    "    print(f\"Random Sampling PageRank Results (n = {SAMPLES})\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")\n",
    "   \n",
    "    print()\n",
    "        \n",
    "    ranks = pagerank2(directory, DAMPING)\n",
    "    print(f\"PageRank Results from Iteration\")\n",
    "    for page in sorted(ranks):\n",
    "        print(f\"  {page}: {ranks[page]:.4f}\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sampling PageRank Results (n = 10000)\n",
      "  1.html: 0.2275\n",
      "  2.html: 0.4300\n",
      "  3.html: 0.2117\n",
      "  4.html: 0.1308\n",
      "\n",
      "PageRank Results from Iteration\n",
      "  1.html: 0.2199\n",
      "  2.html: 0.4292\n",
      "  3.html: 0.2199\n",
      "  4.html: 0.1310\n"
     ]
    }
   ],
   "source": [
    "get('corpus0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sampling PageRank Results (n = 10000)\n",
      "  bfs.html: 0.1188\n",
      "  dfs.html: 0.0857\n",
      "  games.html: 0.2240\n",
      "  minesweeper.html: 0.1137\n",
      "  minimax.html: 0.1291\n",
      "  search.html: 0.2124\n",
      "  tictactoe.html: 0.1163\n",
      "\n",
      "PageRank Results from Iteration\n",
      "  bfs.html: 0.1150\n",
      "  dfs.html: 0.0807\n",
      "  games.html: 0.2278\n",
      "  minesweeper.html: 0.1182\n",
      "  minimax.html: 0.1309\n",
      "  search.html: 0.2091\n",
      "  tictactoe.html: 0.1182\n"
     ]
    }
   ],
   "source": [
    "get('corpus1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sampling PageRank Results (n = 10000)\n",
      "  ai.html: 0.1912\n",
      "  algorithms.html: 0.1045\n",
      "  c.html: 0.1241\n",
      "  inference.html: 0.1301\n",
      "  logic.html: 0.0234\n",
      "  programming.html: 0.2312\n",
      "  python.html: 0.1250\n",
      "  recursion.html: 0.0705\n",
      "\n",
      "PageRank Results from Iteration\n",
      "  ai.html: 0.1887\n",
      "  algorithms.html: 0.1066\n",
      "  c.html: 0.1240\n",
      "  inference.html: 0.1290\n",
      "  logic.html: 0.0264\n",
      "  programming.html: 0.2297\n",
      "  python.html: 0.1240\n",
      "  recursion.html: 0.0716\n"
     ]
    }
   ],
   "source": [
    "get('corpus2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My original incorrect iteration, below I didn't add recursion link to every page so the results were lower than they should have been for the corpus 2 example, though the first two corpus examples worked. \n",
    "\n",
    "**I also iterated using a while loop here. I actually prefer the function iteration method, but the while loop is clean and easy to follow.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_pagerank(directory, d):\n",
    "    corpus = crawl(directory)\n",
    "    start_prob = (1 - d) / len(corpus)\n",
    "    \n",
    "    old_ranks = {page: start_prob for page in corpus}\n",
    "    \n",
    "    # new corpus created with pages and links to p rather than\n",
    "    # pages and links from p\n",
    "    new_corpus = {}\n",
    "    pages = [pg for pg, val in corpus.items()]\n",
    "    for page in pages:\n",
    "        links = {}\n",
    "        for pg, lnk in corpus.items():\n",
    "            if page in lnk:\n",
    "                links.update({pg:len(lnk)})\n",
    "            else:\n",
    "                pass\n",
    "        new_corpus.update({page: links})\n",
    "\n",
    "    while True:\n",
    "        new_ranks = {}       \n",
    "        for page, links in new_corpus.items():\n",
    "            #print(page, links)\n",
    "            if len(links) == 0:\n",
    "                new_ranks.update({page: start_prob})\n",
    "            elif len(links) == 1:\n",
    "                link_page =  list(links.keys())[0]\n",
    "                num_links =  list(links.values())[0]\n",
    "                new_ranks.update({page: start_prob + d * old_ranks[link_page]/num_links})\n",
    "            else:\n",
    "                rank_link_sum = 0\n",
    "                for pg, lnk in links.items():\n",
    "                    rank_link_sum += (old_ranks[pg]/lnk)\n",
    "                    new_ranks.update({page: start_prob + d * rank_link_sum})\n",
    "                                 \n",
    "        # Compare new ranks to old and check for +/- .0001 difference\n",
    "        # if check fails, old_ranks = new_ranks\n",
    "        old_check = sum(old_ranks.values())\n",
    "        new_check = sum(new_ranks.values())\n",
    "        \n",
    "        #old_ranks_vals = [rank for p, rank in old_ranks.items]\n",
    "        diffs = new_check - old_check\n",
    "              \n",
    "        if  diffs <= 0.001:\n",
    "            return(new_ranks)\n",
    "            break\n",
    "        else:\n",
    "            old_ranks = new_ranks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
