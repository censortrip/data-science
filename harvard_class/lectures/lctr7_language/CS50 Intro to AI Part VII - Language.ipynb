{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Artificial Intelligence with Python\n",
    "\n",
    "## Part VII - Language\n",
    "\n",
    "Harvard CS50 Introduction to Artificial Intelligence with Python is an online course that I took in the Spring of 2020. It consisted of 6 lectures of which I have a notebook for each. Each lecture had 2 projects, those are located in the projects folder in the same directory as this notebook.\n",
    "\n",
    "[Course Link](https://cs50.harvard.edu/ai/)\n",
    "\n",
    "[Lecture Link](https://www.youtube.com/watch?v=_hAVVULrZ0Q&list=PLhQjrBD2T382Nz7z1AEXmioc27axa19Kv&index=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Natural Language Processing (NLP)\n",
    "Natural language processing is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. Challenges in natural language processing frequently involve:\n",
    "\n",
    "* Automatic Summarization\n",
    "* Information Extraction\n",
    "* Language Identification\n",
    "* Machine Translation\n",
    "* Named Entity Recognition\n",
    "* Speech Recognition\n",
    "* Text Classification\n",
    "* Word Sense Disambiguation\n",
    "\n",
    "\n",
    "**Syntax** -In linguistics, syntax is the set of rules, principles, and processes that govern the structure of sentences in a given language,\n",
    "\n",
    "**Formal Grammar** - a system of rules for generating sentences in a language\n",
    "\n",
    "**Context-Free Grammar** - a method for generating sentences in a language by replacing words with symbols in order for an ai to be able to recognize word semantics\n",
    "\n",
    "**Context-Free Grammar Examples:**\n",
    "* Initial sentence: she saw the city.\n",
    "* she ---> represented by N for Noun\n",
    "* saw ---> represented by V for Verb\n",
    "* the ---> represented by D for Determiner (the, a, an)\n",
    "* city --> represented by N for Noun\n",
    "* Final sentence structure: N-V-D-N\n",
    "* N, V, D are known as non-terminal symbols, terminal symbols are the actual words\n",
    "\n",
    "Non-terminal symbols are assigned to potential words that can be used with them in any given situation or language. For example:\n",
    "\n",
    "* N ---> she | city | car | Harry | ...\n",
    "* D ---> the | a | an | ...\n",
    "* V ---> saw | ate | walked | ...\n",
    "* P (preposition) ---> to | on | over | ...\n",
    "* ADJ -> blue | busy | old | ...\n",
    "\n",
    "In NLP there are also non-terminal symbols that signify phrases or multiple non-terminal symbols like so:\n",
    "\n",
    "* NP (Noun Phrase) ---> N | D N  <----Means either a Noun or a Determiner followed by a Noun\n",
    "* An example of a noun phrase: the(D) city(N)\n",
    "* VP (Verb Phrase) ---> V | V NP, could be: saw(V) the(D) city(N)\n",
    "* S (Sentence) ---> NP VP <--- a noun phrase followed by a verb phrase\n",
    "\n",
    "Taking all of the above syntax rules, a sentence structure could look like this:\n",
    "\n",
    "<img src='data/lan1.png'>\n",
    "\n",
    "The image above represents a parse tree, note that the top level is the sentence itself (S) followed by a Noun Phrase (NP), which in this case is just a single noun 'she'. The Verb Phrase (VP) is composed of a single verb (V) and a noun phrase (NP) which contains a Determiner (D) and a  Noun(N)\n",
    "\n",
    "**Parse Tree (syntax tree)** - A parse tree or parsing tree or derivation tree or concrete syntax tree is an ordered, rooted tree that represents the syntactic structure of a string according to some context-free grammar. The term parse tree itself is used primarily in computational linguistics; in theoretical syntax, the term syntax tree is more common.\n",
    "\n",
    "Structures like Parse Trees are useful when there are limited word amounts to a given word bank.\n",
    "\n",
    "**ntlt (natural language toolkit)** - is a Python library designed to handle basic nlp tasks such as parsing context-free grammer\n",
    "\n",
    "\n",
    "## Context-Free Grammar Examples using nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: she saw a car\n",
      "         S             \n",
      "  _______|___           \n",
      " |           VP        \n",
      " |    _______|___       \n",
      " NP  |           NP    \n",
      " |   |        ___|___   \n",
      " N   V       D       N \n",
      " |   |       |       |  \n",
      "she saw      a      car\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example parses a sentences into a syntax tree using nltk, note\n",
    "# the bank of words are pre-set\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "\n",
    "    NP -> D N | N\n",
    "    VP -> V | V NP\n",
    "\n",
    "    D -> \"the\" | \"a\"\n",
    "    N -> \"she\" | \"city\" | \"car\"\n",
    "    V -> \"saw\" | \"walked\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sentence = input(\"Sentence: \").split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "        tree.draw()\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: she saw the big city with binoculars\n",
      "     S                                      \n",
      "  ___|_______                                \n",
      " |           VP                             \n",
      " |    _______|_________________              \n",
      " |   |       NP                |            \n",
      " |   |    ___|___              |             \n",
      " |   |   |       NP            PP           \n",
      " |   |   |    ___|___      ____|______       \n",
      " NP  |   |   AP      NP   |           NP    \n",
      " |   |   |   |       |    |           |      \n",
      " N   V   D   A       N    P           N     \n",
      " |   |   |   |       |    |           |      \n",
      "she saw the big     city with     binoculars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This example contains more complex rules than\n",
    "# above, (AP - Adjective Phrases, PP - Prepositional Phrases)\n",
    "\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "\n",
    "    AP -> A | A AP\n",
    "    NP -> N | D NP | AP NP | N PP\n",
    "    PP -> P NP\n",
    "    VP -> V | V NP | V NP PP\n",
    "\n",
    "    A -> \"big\" | \"blue\" | \"small\" | \"dry\" | \"wide\"\n",
    "    D -> \"the\" | \"a\" | \"an\"\n",
    "    N -> \"she\" | \"city\" | \"car\" | \"street\" | \"dog\" | \"binoculars\"\n",
    "    P -> \"on\" | \"over\" | \"before\" | \"below\" | \"with\"\n",
    "    V -> \"saw\" | \"walked\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "sentence = input(\"Sentence: \").split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "        tree.draw()\n",
    "        break\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## n-grams\n",
    "a continguous sequence of n items from a sample of text where n represents the number of continguous words to search for\n",
    "\n",
    "**Character n-gram** - a continguous sequence of n charaters from a sample of text\n",
    "\n",
    "**Word n-gram** - a continguous sequence of n words from a sample of text\n",
    "\n",
    "**Unigram** - a continguous sequence of 1 item from a sample of text (where n=1)\n",
    "\n",
    "**Bigram** - a continguous sequence of 2 items from a sample of text (where n=2)\n",
    "\n",
    "**Trigram** - a continguous sequence of 3 items from a sample of text (where n=3)\n",
    "\n",
    "An example of a trigram would be:\n",
    "* Initial sentence: 'How often have I said to you to stop that?'\n",
    "* Trigram 1: How often have\n",
    "* Trigram 2: often have I\n",
    "* Trigram 3: have I said, and so on\n",
    "\n",
    "breaking down sentences into smaller sections allows an ai to more easily identify structure and meaning. \n",
    "\n",
    "---\n",
    "## Tokenization\n",
    "the task of splitting a sequence of characters into pieces (tokens), for example, python's built-in split() function tokenizes sentences and puts all the tokens into a list \n",
    "\n",
    "### The tokenization example below examines 17 Shelock Holmes stories and locates the most common n-grams from all combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "62: ('it', 'was', 'a')\n",
      "61: ('one', 'of', 'the')\n",
      "53: ('i', 'think', 'that')\n",
      "47: ('out', 'of', 'the')\n",
      "47: ('that', 'he', 'was')\n",
      "46: ('there', 'was', 'a')\n",
      "43: ('it', 'is', 'a')\n",
      "42: ('that', 'i', 'was')\n",
      "42: ('that', 'it', 'was')\n",
      "42: ('that', 'he', 'had')\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Calculate top term frequencies for a corpus of documents.\"\"\"\n",
    "\n",
    "    if len(sys.argv) != 3:\n",
    "        sys.exit(\"Usage: python tfidf.py n corpus\")\n",
    "    print(\"Loading data...\")\n",
    "\n",
    "    n = 3\n",
    "    corpus = load_data('data/holmes')\n",
    "\n",
    "    # Compute n-grams\n",
    "    ngrams = Counter(nltk.ngrams(corpus, n))\n",
    "\n",
    "    # Print most common n-grams\n",
    "    for ngram, freq in ngrams.most_common(10):\n",
    "        print(f\"{freq}: {ngram}\")\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    contents = []\n",
    "\n",
    "    # Read all files and extract words\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            contents.extend([\n",
    "                word.lower() for word in\n",
    "                nltk.word_tokenize(f.read())\n",
    "                if any(c.isalpha() for c in word)\n",
    "            ])\n",
    "    return contents\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**the above n-grams are useful for prediction, knowing the most common trigrams, one could feasibly predict the third word when the first two are seen together**\n",
    "\n",
    "## Markov models can be used for predicting words\n",
    "In the example below, a python library called makovify is used to generate random sentences that sound like Shakespeare (but that aren't specifically used by Shakespeare in his works). Note that the complete works of Shakespeare are used for this to work. This model is just predicting which words are likely to come next based on prior words (n-grams). Note that the sentences don't necessarily have any meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I never Deny your asking: take your way to give breathing to his wish?\n",
      "\n",
      "If that the lips of those broils, Beginning in the incursions, thou strikest as slow as another.\n",
      "\n",
      "Come, cordial and not mean to look upon my sin.\n",
      "\n",
      "Have you a good memory And witness of their death.\n",
      "\n",
      "Not knowing what they swear in both the worlds suffer, Ere we depart we'll share a bounteous time In different beds of flow'rs: Love-thoughts lie rich when canopied with bow'rs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Read text from file\n",
    "with open('data/shakespeare.txt') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Train model\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Generate sentences\n",
    "print()\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Text Categorization\n",
    "Used to classify text, good for determining text sentiment (i.e. positive or negative). Product reviews are example of text where categorization can be used.\n",
    "\n",
    "\n",
    "### Bag-Of-Words Model\n",
    "Model that represents text as an unordered collection of words where syntax is irrelvant just the words and their meanings themselves are relevant\n",
    "\n",
    "### Naive Bayes Model (Similar to Bayes Rule for probality)\n",
    "For text classification purposes we might want to know the probability that a specific piece of text is positive (P(Positive)) or negative (P(Negative)). For text classification purposes, information to help determine positive or negative can be through specific words or text samples\n",
    "\n",
    "Naive Bayes Model Example:\n",
    "* P(positive | 'my grandson loved it'), given the sentence 'my grandson love it, what is the probablity that the message is positive.\n",
    "* The bag-of-words approach will treat the sentence as just a bunch of words like P(positive | 'my', 'grandson', 'loved', 'it)\n",
    "* Bayes Rule, can then be applied to the function like so (see Uncertainty lecture for more info on Bayes Rule):\n",
    "\n",
    "**P('my', 'grandson', 'loved', 'it' | positive) * P(positive) / P('my', 'granson', 'loved', 'it')**\n",
    "\n",
    "Taking the above idea, we can use joint probablity rules to further discern positive and negative messages by multiplying each individual word based on its own positivity/negativity like so:\n",
    "\n",
    "\n",
    "### Explained well 58 minutes into lecture\n",
    "**P(positive) * P('my' | positive) * P('grandson' | positive) * P('loved' | positive) * P('it' | positive)**\n",
    "\n",
    "* The pro of positive P(positive) = # positive samples/ # total samples\n",
    "* prob of P('loved' | positive) = # of positive samples with 'loved' / number of positive samples\n",
    "* all the other probs are calculated the same\n",
    "\n",
    "**Additive Smoothing** - adding a value $\\alpha$ to each value in our distributino to smooth the data (handle words that have 0 probability in a data set) \n",
    "\n",
    "**Laplace Smoothing** - adding 1 to each value in our distribution: pretending we've sen eachv value one more time than we actually have (prevents multiplication by 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: I liked it\n",
      "Positive: 0.7485\n",
      "Negative: 0.2515\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "\n",
    "    # Read data from files\n",
    "    positives, negatives = load_data(filename)\n",
    "\n",
    "    # Create a set of all words\n",
    "    words = set()\n",
    "    for document in positives:\n",
    "        words.update(document)\n",
    "    for document in negatives:\n",
    "        words.update(document)\n",
    "\n",
    "    # Extract features from text\n",
    "    training = []\n",
    "    training.extend(generate_features(positives, words, \"Positive\"))\n",
    "    training.extend(generate_features(negatives, words, \"Negative\"))\n",
    "\n",
    "    # Classify a new sample\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training)\n",
    "    s = input(\"s: \")\n",
    "    result = (classify(classifier, s, words))\n",
    "    for key in result.samples():\n",
    "        print(f\"{key}: {result.prob(key):.4f}\")\n",
    "\n",
    "\n",
    "def extract_words(document):\n",
    "    return set(\n",
    "        word.lower() for word in nltk.word_tokenize(document)\n",
    "        if any(c.isalpha() for c in word)\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    result = []\n",
    "    for filename in [\"positives.txt\", \"negatives.txt\"]:\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "            result.append([\n",
    "                extract_words(line)\n",
    "                for line in f.read().splitlines()\n",
    "            ])\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_features(documents, words, label):\n",
    "    features = []\n",
    "    for document in documents:\n",
    "        features.append(({\n",
    "            word: (word in document)\n",
    "            for word in words\n",
    "        }, label))\n",
    "    return features\n",
    "\n",
    "\n",
    "def classify(classifier, document, words):\n",
    "    document_words = extract_words(document)\n",
    "    features = {\n",
    "        word: (word in document_words)\n",
    "        for word in words\n",
    "    }\n",
    "    return classifier.prob_classify(features)\n",
    "\n",
    "main('data/test_corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Information Retrieval\n",
    "The task of finding relevant documents in response to a user query\n",
    "\n",
    "**Topic Modeling** - models for discovering the topics for a set of documents\n",
    "\n",
    "### Term Frequency (TF)\n",
    "number of times a term appears in a document\n",
    "\n",
    "\n",
    "\n",
    "The below examples use term frequency to find the most common words from a corpus of Shelock Holmes stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words from corpus...\n",
      "bohemia.txt\n",
      "squires.txt\n",
      "carbuncle.txt\n",
      "clerk.txt\n",
      "problem.txt\n",
      "league.txt\n",
      "blaze.txt\n",
      "boscombe.txt\n",
      "coronet.txt\n",
      "ritual.txt\n",
      "twisted.txt\n",
      "interpreter.txt\n",
      "patient.txt\n",
      "engineer.txt\n",
      "speckled.txt\n",
      "face.txt\n",
      "treaty.txt\n",
      "gloria_scott.txt\n",
      "crooked.txt\n",
      "copper.txt\n",
      "bachelor.txt\n",
      "Calculating term frequencies...\n",
      "Computing top terms...\n",
      "\n",
      "bohemia.txt\n",
      "    the: 443.0000\n",
      "    i: 261.0000\n",
      "    and: 254.0000\n",
      "    to: 245.0000\n",
      "    of: 237.0000\n",
      "squires.txt\n",
      "    the: 508.0000\n",
      "    of: 206.0000\n",
      "    and: 169.0000\n",
      "    to: 168.0000\n",
      "    a: 152.0000\n",
      "carbuncle.txt\n",
      "    the: 463.0000\n",
      "    of: 233.0000\n",
      "    a: 208.0000\n",
      "    and: 199.0000\n",
      "    i: 188.0000\n",
      "clerk.txt\n",
      "    the: 312.0000\n",
      "    i: 210.0000\n",
      "    a: 186.0000\n",
      "    and: 180.0000\n",
      "    of: 174.0000\n",
      "problem.txt\n",
      "    the: 427.0000\n",
      "    i: 231.0000\n",
      "    to: 209.0000\n",
      "    of: 191.0000\n",
      "    and: 187.0000\n",
      "league.txt\n",
      "    the: 460.0000\n",
      "    and: 271.0000\n",
      "    i: 264.0000\n",
      "    a: 239.0000\n",
      "    of: 224.0000\n",
      "blaze.txt\n",
      "    the: 641.0000\n",
      "    of: 242.0000\n",
      "    and: 242.0000\n",
      "    a: 242.0000\n",
      "    to: 238.0000\n",
      "boscombe.txt\n",
      "    the: 529.0000\n",
      "    and: 279.0000\n",
      "    i: 273.0000\n",
      "    to: 251.0000\n",
      "    of: 244.0000\n",
      "coronet.txt\n",
      "    the: 466.0000\n",
      "    i: 356.0000\n",
      "    to: 270.0000\n",
      "    and: 238.0000\n",
      "    a: 213.0000\n",
      "ritual.txt\n",
      "    the: 482.0000\n",
      "    of: 255.0000\n",
      "    and: 216.0000\n",
      "    to: 200.0000\n",
      "    i: 200.0000\n",
      "twisted.txt\n",
      "    the: 493.0000\n",
      "    a: 275.0000\n",
      "    and: 270.0000\n",
      "    i: 238.0000\n",
      "    of: 234.0000\n",
      "interpreter.txt\n",
      "    the: 353.0000\n",
      "    and: 188.0000\n",
      "    a: 186.0000\n",
      "    to: 178.0000\n",
      "    i: 153.0000\n",
      "patient.txt\n",
      "    the: 346.0000\n",
      "    i: 187.0000\n",
      "    to: 184.0000\n",
      "    and: 172.0000\n",
      "    of: 171.0000\n",
      "engineer.txt\n",
      "    the: 431.0000\n",
      "    i: 313.0000\n",
      "    and: 250.0000\n",
      "    a: 233.0000\n",
      "    to: 215.0000\n",
      "speckled.txt\n",
      "    the: 600.0000\n",
      "    and: 281.0000\n",
      "    of: 276.0000\n",
      "    a: 252.0000\n",
      "    i: 233.0000\n",
      "face.txt\n",
      "    the: 326.0000\n",
      "    i: 298.0000\n",
      "    and: 226.0000\n",
      "    to: 185.0000\n",
      "    a: 173.0000\n",
      "treaty.txt\n",
      "    the: 688.0000\n",
      "    i: 348.0000\n",
      "    of: 319.0000\n",
      "    and: 318.0000\n",
      "    to: 316.0000\n",
      "gloria_scott.txt\n",
      "    the: 430.0000\n",
      "    and: 273.0000\n",
      "    of: 220.0000\n",
      "    a: 203.0000\n",
      "    i: 188.0000\n",
      "crooked.txt\n",
      "    the: 438.0000\n",
      "    and: 204.0000\n",
      "    of: 199.0000\n",
      "    i: 184.0000\n",
      "    a: 175.0000\n",
      "copper.txt\n",
      "    the: 485.0000\n",
      "    i: 330.0000\n",
      "    and: 275.0000\n",
      "    to: 256.0000\n",
      "    a: 238.0000\n",
      "bachelor.txt\n",
      "    the: 401.0000\n",
      "    i: 236.0000\n",
      "    and: 234.0000\n",
      "    to: 233.0000\n",
      "    a: 211.0000\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "    \"\"\"Calculate top term frequencies for a corpus of documents.\"\"\"\n",
    "    corpus = load_data(filename)\n",
    "\n",
    "    # Get all words in corpus\n",
    "    print(\"Extracting words from corpus...\")\n",
    "    words = set()\n",
    "    for filename in corpus:\n",
    "        print(filename)\n",
    "        words.update(corpus[filename])\n",
    "        \n",
    "    # Calculate TF-IDFs\n",
    "    print(\"Calculating term frequencies...\")\n",
    "    tfidfs = dict()\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename] = []\n",
    "        for word in corpus[filename]:\n",
    "            tf = corpus[filename][word]\n",
    "            tfidfs[filename].append((word, tf))\n",
    "\n",
    "    # Sort and get top 5 term frequencies for each file\n",
    "    print(\"Computing top terms...\")\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename].sort(key=lambda tfidf: tfidf[1], reverse=True)\n",
    "        tfidfs[filename] = tfidfs[filename][:5]\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    for filename in corpus:\n",
    "        print(filename)\n",
    "        for term, score in tfidfs[filename]:\n",
    "            print(f\"    {term}: {score:.4f}\")\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    files = dict()\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "\n",
    "            # Extract words\n",
    "            contents = [\n",
    "                word.lower() for word in\n",
    "                nltk.word_tokenize(f.read())\n",
    "                if word.isalpha()\n",
    "            ]\n",
    "\n",
    "            # Count frequencies\n",
    "            frequencies = dict()\n",
    "            for word in contents:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "            files[filename] = frequencies\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "main('data/holmes2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are not very useful as they just return the most common words from all the Sherlock Holmes stories which happen to be very common words in English like I, the, and a These words are known as **function words**\n",
    "\n",
    "**Function Words** - words that have little meaning on their own, but are used to grammatically connect other words (am, by, the, a, an, to, etc.)\n",
    "\n",
    "**Content Words** - words that have meaining on their own (algorithm, category, computer)\n",
    "\n",
    "The example below takes a text file containing a number of common function words and uses it in order to end up with content words rather than function words for the most frequent word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words from corpus...\n",
      "Calculating term frequencies...\n",
      "Computing top terms...\n",
      "\n",
      "bohemia.txt\n",
      "    holmes: 48.0000\n",
      "    man: 22.0000\n",
      "    photograph: 21.0000\n",
      "    own: 18.0000\n",
      "    street: 18.0000\n",
      "squires.txt\n",
      "    holmes: 53.0000\n",
      "    man: 35.0000\n",
      "    cunningham: 31.0000\n",
      "    inspector: 29.0000\n",
      "    colonel: 25.0000\n",
      "carbuncle.txt\n",
      "    man: 39.0000\n",
      "    holmes: 38.0000\n",
      "    hat: 27.0000\n",
      "    see: 27.0000\n",
      "    goose: 26.0000\n",
      "clerk.txt\n",
      "    holmes: 26.0000\n",
      "    pycroft: 23.0000\n",
      "    man: 22.0000\n",
      "    little: 17.0000\n",
      "    mawson: 17.0000\n",
      "problem.txt\n",
      "    holmes: 28.0000\n",
      "    watson: 22.0000\n",
      "    man: 21.0000\n",
      "    moriarty: 20.0000\n",
      "    away: 15.0000\n",
      "league.txt\n",
      "    holmes: 53.0000\n",
      "    man: 26.0000\n",
      "    little: 25.0000\n",
      "    see: 23.0000\n",
      "    wilson: 22.0000\n",
      "blaze.txt\n",
      "    holmes: 51.0000\n",
      "    horse: 50.0000\n",
      "    colonel: 44.0000\n",
      "    straker: 38.0000\n",
      "    inspector: 21.0000\n",
      "boscombe.txt\n",
      "    holmes: 46.0000\n",
      "    man: 43.0000\n",
      "    mccarthy: 37.0000\n",
      "    father: 33.0000\n",
      "    son: 28.0000\n",
      "coronet.txt\n",
      "    coronet: 27.0000\n",
      "    holmes: 27.0000\n",
      "    man: 27.0000\n",
      "    think: 26.0000\n",
      "    house: 23.0000\n",
      "ritual.txt\n",
      "    musgrave: 26.0000\n",
      "    old: 26.0000\n",
      "    brunton: 23.0000\n",
      "    man: 22.0000\n",
      "    two: 20.0000\n",
      "twisted.txt\n",
      "    man: 31.0000\n",
      "    holmes: 29.0000\n",
      "    clair: 27.0000\n",
      "    little: 21.0000\n",
      "    face: 20.0000\n",
      "interpreter.txt\n",
      "    holmes: 29.0000\n",
      "    man: 29.0000\n",
      "    way: 22.0000\n",
      "    brother: 21.0000\n",
      "    mycroft: 21.0000\n",
      "patient.txt\n",
      "    holmes: 39.0000\n",
      "    man: 28.0000\n",
      "    blessington: 26.0000\n",
      "    doctor: 18.0000\n",
      "    street: 16.0000\n",
      "engineer.txt\n",
      "    time: 25.0000\n",
      "    little: 25.0000\n",
      "    colonel: 20.0000\n",
      "    door: 19.0000\n",
      "    came: 18.0000\n",
      "speckled.txt\n",
      "    holmes: 56.0000\n",
      "    room: 25.0000\n",
      "    see: 22.0000\n",
      "    sister: 21.0000\n",
      "    roylott: 20.0000\n",
      "face.txt\n",
      "    face: 24.0000\n",
      "    cottage: 24.0000\n",
      "    man: 23.0000\n",
      "    little: 23.0000\n",
      "    wife: 23.0000\n",
      "treaty.txt\n",
      "    holmes: 67.0000\n",
      "    phelps: 39.0000\n",
      "    room: 36.0000\n",
      "    come: 34.0000\n",
      "    came: 28.0000\n",
      "gloria_scott.txt\n",
      "    man: 28.0000\n",
      "    know: 24.0000\n",
      "    trevor: 23.0000\n",
      "    came: 19.0000\n",
      "    hudson: 17.0000\n",
      "crooked.txt\n",
      "    barclay: 34.0000\n",
      "    man: 27.0000\n",
      "    colonel: 20.0000\n",
      "    know: 18.0000\n",
      "    room: 18.0000\n",
      "copper.txt\n",
      "    holmes: 43.0000\n",
      "    rucastle: 38.0000\n",
      "    little: 37.0000\n",
      "    miss: 36.0000\n",
      "    man: 34.0000\n",
      "bachelor.txt\n",
      "    simon: 40.0000\n",
      "    lord: 35.0000\n",
      "    holmes: 34.0000\n",
      "    little: 26.0000\n",
      "    lady: 24.0000\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "    \"\"\"\n",
    "    Calculate top term frequencies for a corpus of documents.\n",
    "    Excludes stop words.\n",
    "    \"\"\"\n",
    "    corpus = load_data(filename)\n",
    "\n",
    "    # Get all words in corpus\n",
    "    print(\"Extracting words from corpus...\")\n",
    "    words = set()\n",
    "    for filename in corpus:\n",
    "        words.update(corpus[filename])\n",
    "\n",
    "    # Calculate TF-IDFs\n",
    "    print(\"Calculating term frequencies...\")\n",
    "    tfidfs = dict()\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename] = []\n",
    "        for word in corpus[filename]:\n",
    "            tf = corpus[filename][word]\n",
    "            tfidfs[filename].append((word, tf))\n",
    "\n",
    "    # Sort and get top 5 term frequencies for each file\n",
    "    print(\"Computing top terms...\")\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename].sort(key=lambda tfidf: tfidf[1], reverse=True)\n",
    "        tfidfs[filename] = tfidfs[filename][:5]\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    for filename in corpus:\n",
    "        print(filename)\n",
    "        for term, score in tfidfs[filename]:\n",
    "            print(f\"    {term}: {score:.4f}\")\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "\n",
    "    with open(\"data/function_words.txt\") as f:\n",
    "        function_words = set(f.read().splitlines())\n",
    "\n",
    "    files = dict()\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "\n",
    "            # Extract words\n",
    "            contents = [\n",
    "                word.lower() for word in\n",
    "                nltk.word_tokenize(f.read())\n",
    "                if word.isalpha()\n",
    "            ]\n",
    "\n",
    "            # Count frequencies\n",
    "            frequencies = dict()\n",
    "            for word in contents:\n",
    "\n",
    "                if word in function_words:\n",
    "                    continue\n",
    "                elif word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "            files[filename] = frequencies\n",
    "\n",
    "    return files\n",
    "\n",
    "main('data/holmes2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above methodology of weeding out function words is a step in the right direction, it still does not really get to the bottom of what each story is really about other than that they are Sherlock Holmes stories. In order to actually discern differences bewteen the stories, words between the stories must also be compared\n",
    "\n",
    "**Inverse Document Frequency (IDF)** - measures how common or rare a word is across a corpus of documents calculated by:\n",
    "* log(total documents / number of documents containing the word), note with this formula if all documents contain a specific word the result is zero, which means that it is not rare, higher values result in less common words\n",
    "\n",
    "**tf-idf** - ranking of what words are important in a document by multiplying term frequency(TF) by inverse document frequency(IDF)\n",
    "\n",
    "In the below example, tf-idf is used to find the mose important words per story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words from corpus...\n",
      "Calculating inverse document frequencies...\n",
      "Calculating term frequencies...\n",
      "Computing top terms...\n",
      "\n",
      "bohemia.txt\n",
      "    majesty: 54.8014\n",
      "    briony: 33.4897\n",
      "    irene: 32.9193\n",
      "    adler: 30.5679\n",
      "    photograph: 26.3080\n",
      "squires.txt\n",
      "    cunningham: 94.3802\n",
      "    alec: 57.8459\n",
      "    acton: 45.6678\n",
      "    william: 31.5063\n",
      "    colonel: 31.3191\n",
      "carbuncle.txt\n",
      "    goose: 61.1358\n",
      "    geese: 51.7569\n",
      "    horner: 39.5788\n",
      "    ryder: 36.5343\n",
      "    peterson: 33.4897\n",
      "clerk.txt\n",
      "    pycroft: 70.0240\n",
      "    mawson: 51.7569\n",
      "    pinner: 25.8651\n",
      "    hardware: 21.3117\n",
      "    birmingham: 21.1624\n",
      "problem.txt\n",
      "    moriarty: 60.8904\n",
      "    professor: 28.2165\n",
      "    spray: 18.2671\n",
      "    rock: 18.2671\n",
      "    meiringen: 15.2226\n",
      "league.txt\n",
      "    wilson: 42.8100\n",
      "    league: 37.6220\n",
      "    merryweather: 36.5343\n",
      "    jones: 33.4897\n",
      "    assistant: 32.9193\n",
      "blaze.txt\n",
      "    straker: 115.6919\n",
      "    colonel: 55.1216\n",
      "    horse: 54.9306\n",
      "    trainer: 51.7569\n",
      "    moor: 48.7124\n",
      "boscombe.txt\n",
      "    mccarthy: 112.6473\n",
      "    lestrade: 56.4330\n",
      "    turner: 49.3789\n",
      "    boscombe: 45.6678\n",
      "    pool: 42.3248\n",
      "coronet.txt\n",
      "    coronet: 82.2021\n",
      "    arthur: 44.6761\n",
      "    gems: 39.5788\n",
      "    holder: 29.8481\n",
      "    snow: 23.5138\n",
      "ritual.txt\n",
      "    brunton: 70.0240\n",
      "    musgrave: 61.1358\n",
      "    ritual: 35.2706\n",
      "    hurlstone: 27.4007\n",
      "    butler: 21.5263\n",
      "twisted.txt\n",
      "    clair: 82.2021\n",
      "    neville: 57.8459\n",
      "    lascar: 36.5343\n",
      "    opium: 25.8651\n",
      "    whitney: 24.3562\n",
      "interpreter.txt\n",
      "    melas: 57.8459\n",
      "    mycroft: 49.3789\n",
      "    greek: 37.6220\n",
      "    interpreter: 33.4897\n",
      "    latimer: 21.3117\n",
      "patient.txt\n",
      "    blessington: 79.1576\n",
      "    trevelyan: 48.7124\n",
      "    brook: 24.3562\n",
      "    consultation: 15.2226\n",
      "    resident: 14.1083\n",
      "engineer.txt\n",
      "    hydraulic: 30.4452\n",
      "    stark: 27.4007\n",
      "    eyford: 27.4007\n",
      "    colonel: 25.0553\n",
      "    engineer: 24.3562\n",
      "speckled.txt\n",
      "    roylott: 60.8904\n",
      "    stoner: 57.8459\n",
      "    ventilator: 42.6233\n",
      "    stepfather: 36.5343\n",
      "    stoke: 33.4897\n",
      "face.txt\n",
      "    cottage: 56.4330\n",
      "    munro: 18.8110\n",
      "    jack: 18.2405\n",
      "    grant: 16.4596\n",
      "    effie: 15.2226\n",
      "treaty.txt\n",
      "    phelps: 118.7364\n",
      "    joseph: 70.0240\n",
      "    harrison: 60.8904\n",
      "    holdhurst: 42.6233\n",
      "    woking: 42.6233\n",
      "gloria_scott.txt\n",
      "    trevor: 70.0240\n",
      "    beddoes: 33.4897\n",
      "    hudson: 24.3964\n",
      "    prendergast: 21.3117\n",
      "    boat: 18.8110\n",
      "crooked.txt\n",
      "    barclay: 103.5138\n",
      "    colonel: 25.0553\n",
      "    aldershot: 18.8110\n",
      "    nancy: 18.2671\n",
      "    regiment: 14.1083\n",
      "copper.txt\n",
      "    rucastle: 115.6919\n",
      "    hunter: 51.7303\n",
      "    toller: 45.6678\n",
      "    copper: 25.8651\n",
      "    beeches: 25.8651\n",
      "bachelor.txt\n",
      "    simon: 121.7809\n",
      "    doran: 36.5343\n",
      "    lestrade: 32.9193\n",
      "    wedding: 30.5679\n",
      "    lord: 29.6554\n"
     ]
    }
   ],
   "source": [
    "def main(filename):\n",
    "    \"\"\"Calculate top TF-IDF for a corpus of documents.\"\"\"\n",
    "    corpus = load_data(filename)\n",
    "\n",
    "    # Get all words in corpus\n",
    "    print(\"Extracting words from corpus...\")\n",
    "    words = set()\n",
    "    for filename in corpus:\n",
    "        words.update(corpus[filename])\n",
    "\n",
    "    # Calculate IDFs\n",
    "    print(\"Calculating inverse document frequencies...\")\n",
    "    idfs = dict()\n",
    "    for word in words:\n",
    "        f = sum(word in corpus[filename] for filename in corpus)\n",
    "        idf = math.log(len(corpus) / f)\n",
    "        idfs[word] = idf\n",
    "\n",
    "    # Calculate TF-IDFs\n",
    "    print(\"Calculating term frequencies...\")\n",
    "    tfidfs = dict()\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename] = []\n",
    "        for word in corpus[filename]:\n",
    "            tf = corpus[filename][word]\n",
    "            \n",
    "            # HERE IS WHERE TF-IDF IS CALCULATED\n",
    "            tfidfs[filename].append((word, tf * idfs[word]))\n",
    "\n",
    "    # Sort and get top 5 TF-IDFs for each file\n",
    "    print(\"Computing top terms...\")\n",
    "    for filename in corpus:\n",
    "        tfidfs[filename].sort(key=lambda tfidf: tfidf[1], reverse=True)\n",
    "        tfidfs[filename] = tfidfs[filename][:5]\n",
    "\n",
    "    # Print results\n",
    "    print()\n",
    "    for filename in corpus:\n",
    "        print(filename)\n",
    "        for term, score in tfidfs[filename]:\n",
    "            print(f\"    {term}: {score:.4f}\")\n",
    "\n",
    "\n",
    "def load_data(directory):\n",
    "    files = dict()\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename)) as f:\n",
    "\n",
    "            # Extract words\n",
    "            contents = [\n",
    "                word.lower() for word in\n",
    "                nltk.word_tokenize(f.read())\n",
    "                if word.isalpha()\n",
    "            ]\n",
    "\n",
    "            # Count frequencies\n",
    "            frequencies = dict()\n",
    "            for word in contents:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "            files[filename] = frequencies\n",
    "\n",
    "    return files\n",
    "\n",
    "main('data/holmes2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting words from corpus...\n",
      "Calculating inverse document frequencies...\n",
      "Calculating term frequencies...\n",
      "Computing top terms...\n",
      "\n",
      "73.hamilton.txt\n",
      "    negative: 18.0359\n",
      "    executive: 15.0754\n",
      "    his: 9.5552\n",
      "    resolutions: 8.1790\n",
      "    preventing: 8.1790\n",
      "37.madison.txt\n",
      "    obscurity: 15.2818\n",
      "    difficulties: 10.0204\n",
      "    predetermination: 8.8853\n",
      "    allowances: 8.8853\n",
      "    organ: 8.8853\n",
      "18.madison.txt\n",
      "    macedon: 41.2445\n",
      "    cities: 37.4511\n",
      "    achaeans: 33.7455\n",
      "    amphictyonic: 22.2133\n",
      "    philip: 22.2133\n",
      "58.disputed.txt\n",
      "    representatives: 14.5816\n",
      "    majority: 9.4764\n",
      "    coalition: 9.1691\n",
      "    reapportionments: 8.8853\n",
      "    house: 8.5153\n",
      "12.hamilton.txt\n",
      "    patrols: 13.3280\n",
      "    contraband: 13.3280\n",
      "    revenue: 12.1645\n",
      "    trade: 10.4076\n",
      "    duties: 10.1168\n",
      "56.disputed.txt\n",
      "    knowledge: 17.3630\n",
      "    information: 14.9821\n",
      "    representatives: 9.4780\n",
      "    eight: 9.1691\n",
      "    hundred: 9.0180\n",
      "29.hamilton.txt\n",
      "    militia: 46.8934\n",
      "    disciplining: 13.3280\n",
      "    posse: 13.3280\n",
      "    comitatus: 13.3280\n",
      "    army: 11.5754\n",
      "59.hamilton.txt\n",
      "    elections: 18.0359\n",
      "    senators: 13.1439\n",
      "    legislatures: 9.0780\n",
      "    risk: 7.4902\n",
      "    exclusive: 6.6803\n",
      "21.hamilton.txt\n",
      "    guaranty: 21.3945\n",
      "    lands: 14.1661\n",
      "    inequalities: 11.2485\n",
      "    total: 10.2238\n",
      "    wealth: 10.0204\n",
      "16.hamilton.txt\n",
      "    delinquency: 8.8853\n",
      "    associates: 8.8853\n",
      "    idle: 7.4990\n",
      "    refractory: 6.6881\n",
      "    unconstitutional: 6.6881\n",
      "66.hamilton.txt\n",
      "    senate: 23.1993\n",
      "    impeachments: 17.9634\n",
      "    thirds: 11.8160\n",
      "    punishment: 10.2238\n",
      "    court: 8.9893\n",
      "15.madison.txt\n",
      "    you: 22.4543\n",
      "    penalty: 13.3280\n",
      "    accounted: 8.8853\n",
      "    resolutions: 8.1790\n",
      "    disobedience: 7.4990\n",
      "20.madison.txt\n",
      "    provinces: 28.3585\n",
      "    provincial: 17.7706\n",
      "    stadtholder: 17.7706\n",
      "    says: 9.4528\n",
      "    province: 8.9817\n",
      "72.hamilton.txt\n",
      "    exclusion: 22.4923\n",
      "    he: 14.8577\n",
      "    his: 13.3773\n",
      "    ill: 10.7003\n",
      "    station: 8.9817\n",
      "01.hamilton.txt\n",
      "    your: 28.3321\n",
      "    you: 15.7180\n",
      "    my: 10.4573\n",
      "    appearance: 9.0180\n",
      "    efficiency: 7.4990\n",
      "36.hamilton.txt\n",
      "    taxation: 15.0306\n",
      "    taxes: 12.4182\n",
      "    knowledge: 11.5754\n",
      "    revenue: 10.8129\n",
      "    double: 9.4528\n",
      "63.disputed.txt\n",
      "    senate: 23.1993\n",
      "    elected: 14.8677\n",
      "    annually: 13.3762\n",
      "    sparta: 12.2254\n",
      "    carthage: 11.2485\n",
      "06.hamilton.txt\n",
      "    wars: 15.0216\n",
      "    carthage: 14.9980\n",
      "    commercial: 10.4076\n",
      "    nations: 8.5913\n",
      "    contests: 8.4996\n",
      "81.hamilton.txt\n",
      "    court: 37.4553\n",
      "    appellate: 36.7844\n",
      "    courts: 35.7024\n",
      "    jury: 25.9953\n",
      "    jurisdiction: 20.8291\n",
      "03.jay.txt\n",
      "    governing: 9.1691\n",
      "    temptations: 7.9527\n",
      "    punish: 7.9527\n",
      "    causes: 7.0984\n",
      "    affords: 6.7363\n",
      "02.jay.txt\n",
      "    providence: 13.3280\n",
      "    recommended: 9.4528\n",
      "    farewell: 8.8853\n",
      "    advised: 7.4990\n",
      "    tried: 6.7363\n",
      "27.hamilton.txt\n",
      "    administered: 8.1790\n",
      "    obedience: 6.1127\n",
      "    reasons: 5.7341\n",
      "    meet: 5.3018\n",
      "    sight: 4.7264\n",
      "08.hamilton.txt\n",
      "    armies: 18.0359\n",
      "    disciplined: 15.2818\n",
      "    military: 14.6853\n",
      "    establishments: 9.6566\n",
      "    soldiery: 8.8853\n",
      "19.madison.txt\n",
      "    emperor: 36.6763\n",
      "    diet: 28.3321\n",
      "    imperial: 23.4083\n",
      "    empire: 22.3701\n",
      "    cantons: 20.0642\n",
      "23.hamilton.txt\n",
      "    defense: 8.8836\n",
      "    build: 7.4990\n",
      "    equip: 7.4990\n",
      "    convince: 6.6881\n",
      "    fleets: 6.4202\n",
      "80.hamilton.txt\n",
      "    controversies: 18.5562\n",
      "    equity: 15.9054\n",
      "    courts: 13.9705\n",
      "    grants: 13.2545\n",
      "    citizens: 12.2044\n",
      "50.disputed.txt\n",
      "    appeals: 11.3329\n",
      "    council: 9.7869\n",
      "    recent: 9.1691\n",
      "    censorial: 8.8853\n",
      "    extinction: 8.8853\n",
      "65.hamilton.txt\n",
      "    court: 22.4732\n",
      "    impeachments: 20.2088\n",
      "    accused: 11.2485\n",
      "    prosecution: 10.6036\n",
      "    impeachment: 9.4528\n",
      "84.hamilton.txt\n",
      "    bill: 24.6997\n",
      "    extra: 17.7706\n",
      "    rights: 16.3548\n",
      "    expense: 16.2193\n",
      "    press: 15.2818\n",
      "70.hamilton.txt\n",
      "    plurality: 24.4509\n",
      "    executive: 21.8594\n",
      "    council: 19.5738\n",
      "    consuls: 15.9054\n",
      "    unity: 14.1661\n",
      "35.hamilton.txt\n",
      "    merchant: 19.8325\n",
      "    manufacturing: 16.9993\n",
      "    merchants: 14.1661\n",
      "    consumer: 13.3280\n",
      "    importing: 13.3280\n",
      "33.hamilton.txt\n",
      "    clause: 13.7042\n",
      "    supremacy: 11.3329\n",
      "    pursuant: 10.0321\n",
      "    supreme: 8.0693\n",
      "    taxes: 7.7614\n",
      "77.hamilton.txt\n",
      "    nomination: 17.4772\n",
      "    senate: 14.1213\n",
      "    president: 11.3814\n",
      "    convening: 10.0321\n",
      "    council: 9.7869\n",
      "44.madison.txt\n",
      "    enumeration: 21.2071\n",
      "    coin: 15.2818\n",
      "    silver: 12.2254\n",
      "    gold: 11.3329\n",
      "    excepted: 11.3329\n",
      "78.hamilton.txt\n",
      "    courts: 27.9410\n",
      "    judges: 16.5838\n",
      "    void: 14.1661\n",
      "    judiciary: 13.5161\n",
      "    judicial: 13.3605\n",
      "57.disputed.txt\n",
      "    representatives: 14.5816\n",
      "    choosing: 12.2254\n",
      "    five: 11.2271\n",
      "    representative: 10.1168\n",
      "    six: 9.3885\n",
      "11.hamilton.txt\n",
      "    trade: 22.5498\n",
      "    markets: 22.4707\n",
      "    navigation: 21.2689\n",
      "    navy: 17.9634\n",
      "    ships: 16.7202\n",
      "38.madison.txt\n",
      "    patient: 22.2133\n",
      "    lycurgus: 14.9980\n",
      "    substituted: 14.1661\n",
      "    indefinite: 12.4837\n",
      "    solon: 11.2485\n",
      "25.hamilton.txt\n",
      "    respected: 9.1691\n",
      "    armies: 9.0180\n",
      "    lysander: 8.8853\n",
      "    prepare: 7.4990\n",
      "    admiral: 7.4990\n",
      "74.hamilton.txt\n",
      "    pardoning: 10.0321\n",
      "    treason: 9.1691\n",
      "    connivance: 8.8853\n",
      "    guilt: 7.9527\n",
      "    clemency: 7.4990\n",
      "04.jay.txt\n",
      "    fleet: 14.9980\n",
      "    navigation: 14.1793\n",
      "    obeyed: 11.2485\n",
      "    militia: 10.8216\n",
      "    wales: 8.8853\n",
      "43.madison.txt\n",
      "    compact: 14.9805\n",
      "    validity: 14.1661\n",
      "    cession: 11.2485\n",
      "    republican: 10.3213\n",
      "    consent: 9.4764\n",
      "42.madison.txt\n",
      "    coin: 30.5636\n",
      "    felonies: 22.2133\n",
      "    consuls: 21.2071\n",
      "    piracies: 17.7706\n",
      "    residence: 16.7202\n",
      "22.hamilton.txt\n",
      "    majority: 16.5838\n",
      "    quotas: 9.4528\n",
      "    nine: 9.4528\n",
      "    determinations: 9.1691\n",
      "    minority: 8.9817\n",
      "26.hamilton.txt\n",
      "    enough: 10.8660\n",
      "    army: 10.1284\n",
      "    military: 9.7902\n",
      "    standing: 9.0180\n",
      "    armies: 9.0180\n",
      "09.hamilton.txt\n",
      "    confederate: 17.9634\n",
      "    montesquieu: 13.3762\n",
      "    internal: 8.6815\n",
      "    association: 8.5603\n",
      "    republics: 8.0472\n",
      "79.hamilton.txt\n",
      "    judges: 10.6610\n",
      "    compensation: 9.1691\n",
      "    inability: 9.1691\n",
      "    insanity: 8.8853\n",
      "    subsistence: 7.4990\n",
      "82.hamilton.txt\n",
      "    courts: 41.9115\n",
      "    appellate: 16.7202\n",
      "    court: 16.4803\n",
      "    tribunals: 15.7180\n",
      "    jurisdiction: 15.6218\n",
      "14.madison.txt\n",
      "    democracy: 16.7202\n",
      "    you: 15.7180\n",
      "    your: 14.1661\n",
      "    atlantic: 12.2254\n",
      "    miles: 12.2254\n",
      "64.jay.txt\n",
      "    treaties: 25.6993\n",
      "    secrecy: 15.2818\n",
      "    tides: 13.3280\n",
      "    president: 12.6460\n",
      "    despatch: 11.3329\n",
      "30.hamilton.txt\n",
      "    loans: 13.3280\n",
      "    sums: 11.2485\n",
      "    necessities: 10.2238\n",
      "    funds: 10.0321\n",
      "    diverting: 8.8853\n",
      "53.disputed.txt\n",
      "    elections: 25.2503\n",
      "    knowledge: 18.8099\n",
      "    annual: 14.9804\n",
      "    biennial: 12.2254\n",
      "    year: 11.6904\n",
      "54.disputed.txt\n",
      "    slaves: 36.7844\n",
      "    property: 21.4982\n",
      "    representation: 17.7044\n",
      "    inhabitants: 15.3794\n",
      "    southern: 14.1661\n",
      "61.hamilton.txt\n",
      "    elections: 16.2323\n",
      "    albany: 11.2485\n",
      "    city: 10.6036\n",
      "    counties: 8.5603\n",
      "    electors: 7.8310\n",
      "41.madison.txt\n",
      "    her: 17.1329\n",
      "    loves: 13.3280\n",
      "    armies: 12.6252\n",
      "    europe: 12.4182\n",
      "    liberties: 10.1284\n",
      "10.hamilton.txt\n",
      "    faction: 23.9714\n",
      "    majority: 14.2147\n",
      "    democracy: 13.3762\n",
      "    controlling: 13.3280\n",
      "    republic: 12.8755\n",
      "67.hamilton.txt\n",
      "    vacancies: 36.6763\n",
      "    recess: 23.4083\n",
      "    appointments: 18.6274\n",
      "    session: 18.5562\n",
      "    senate: 15.1300\n",
      "76.hamilton.txt\n",
      "    nomination: 27.4642\n",
      "    assembly: 11.5754\n",
      "    president: 10.1168\n",
      "    senate: 10.0866\n",
      "    party: 10.0866\n",
      "47.madison.txt\n",
      "    executive: 45.2263\n",
      "    department: 44.4179\n",
      "    judiciary: 32.4386\n",
      "    justices: 26.2465\n",
      "    legislative: 24.7874\n",
      "40.madison.txt\n",
      "    confederation: 23.5288\n",
      "    alterations: 22.4707\n",
      "    annapolis: 17.7706\n",
      "    confirmed: 17.7706\n",
      "    recommendatory: 17.7706\n",
      "75.hamilton.txt\n",
      "    treaties: 14.6853\n",
      "    thirds: 14.1793\n",
      "    senate: 12.1040\n",
      "    president: 8.8522\n",
      "    making: 7.9857\n",
      "39.madison.txt\n",
      "    partly: 22.4970\n",
      "    indirectly: 16.7202\n",
      "    majority: 13.0301\n",
      "    again: 10.7003\n",
      "    republican: 10.3213\n",
      "71.hamilton.txt\n",
      "    firmness: 13.4726\n",
      "    duration: 11.0140\n",
      "    complaisance: 9.1691\n",
      "    he: 8.0003\n",
      "    executive: 7.5377\n",
      "49.disputed.txt\n",
      "    appeals: 14.1661\n",
      "    departments: 13.4615\n",
      "    philosophers: 6.6881\n",
      "    prepossessions: 6.1127\n",
      "    quoted: 5.6664\n",
      "05.jay.txt\n",
      "    confederacies: 10.4573\n",
      "    nations: 9.4505\n",
      "    northern: 9.1691\n",
      "    her: 8.5664\n",
      "    your: 8.4996\n",
      "24.hamilton.txt\n",
      "    garrisons: 16.9993\n",
      "    settlements: 13.3280\n",
      "    establishments: 11.2661\n",
      "    he: 10.2861\n",
      "    spain: 9.4528\n",
      "32.hamilton.txt\n",
      "    clause: 19.5774\n",
      "    exports: 18.5562\n",
      "    imports: 17.1205\n",
      "    tax: 12.2685\n",
      "    exclusive: 11.6904\n",
      "55.disputed.txt\n",
      "    hundred: 18.0359\n",
      "    thousand: 11.2662\n",
      "    representatives: 10.9362\n",
      "    population: 8.9817\n",
      "    years: 8.8836\n",
      "45.madison.txt\n",
      "    officers: 10.6610\n",
      "    collectors: 8.8853\n",
      "    compare: 7.9527\n",
      "    justices: 7.4990\n",
      "    enjoy: 6.7363\n",
      "85.hamilton.txt\n",
      "    amendments: 21.3945\n",
      "    amendment: 13.2545\n",
      "    your: 11.3329\n",
      "    adoption: 11.2662\n",
      "    fourths: 11.2485\n",
      "51.disputed.txt\n",
      "    sects: 11.2485\n",
      "    weaker: 11.2271\n",
      "    department: 11.1045\n",
      "    rigorously: 8.8853\n",
      "    angels: 8.8853\n",
      "13.hamilton.txt\n",
      "    her: 12.2378\n",
      "    northern: 12.2254\n",
      "    southern: 8.4996\n",
      "    lists: 6.6881\n",
      "    confederacies: 6.5358\n",
      "83.hamilton.txt\n",
      "    jury: 125.2501\n",
      "    trial: 61.4364\n",
      "    courts: 38.8070\n",
      "    juries: 33.4404\n",
      "    criminal: 31.1653\n",
      "46.madison.txt\n",
      "    governments: 10.7892\n",
      "    prepossessions: 9.1691\n",
      "    militia: 9.0180\n",
      "    federal: 7.7874\n",
      "    everything: 7.4990\n",
      "69.hamilton.txt\n",
      "    governor: 47.1540\n",
      "    president: 30.3503\n",
      "    king: 24.4101\n",
      "    treason: 15.2818\n",
      "    britain: 11.0953\n",
      "07.hamilton.txt\n",
      "    connecticut: 17.4772\n",
      "    debt: 12.2254\n",
      "    lands: 11.3329\n",
      "    apportionment: 11.3329\n",
      "    dispute: 11.2485\n",
      "28.hamilton.txt\n",
      "    usurpers: 9.1691\n",
      "    militia: 9.0180\n",
      "    resistance: 8.5603\n",
      "    parcels: 7.4990\n",
      "    maintain: 6.4378\n",
      "17.hamilton.txt\n",
      "    vassals: 14.9980\n",
      "    feudal: 10.0321\n",
      "    nobles: 10.0321\n",
      "    barons: 9.1691\n",
      "    feudatories: 8.8853\n",
      "48.madison.txt\n",
      "    departments: 14.6853\n",
      "    executive: 14.3217\n",
      "    judiciary: 12.1645\n",
      "    department: 11.1045\n",
      "    legislative: 11.0166\n",
      "31.hamilton.txt\n",
      "    taxation: 10.0204\n",
      "    divisibility: 8.8853\n",
      "    procuring: 8.8853\n",
      "    exigencies: 7.8310\n",
      "    sciences: 7.4990\n",
      "52.disputed.txt\n",
      "    elections: 23.4467\n",
      "    biennial: 15.2818\n",
      "    parliaments: 13.3280\n",
      "    ii: 10.0321\n",
      "    septennial: 10.0321\n",
      "60.hamilton.txt\n",
      "    landed: 18.3381\n",
      "    discrimination: 15.2818\n",
      "    preference: 13.1439\n",
      "    class: 10.3213\n",
      "    partiality: 9.9870\n",
      "62.disputed.txt\n",
      "    senate: 10.0866\n",
      "    deficient: 10.0321\n",
      "    senators: 9.3885\n",
      "    infirmity: 8.8853\n",
      "    inconstant: 8.8853\n",
      "34.hamilton.txt\n",
      "    clause: 13.7042\n",
      "    supremacy: 11.3329\n",
      "    pursuant: 10.0321\n",
      "    supreme: 8.0693\n",
      "    taxes: 7.7614\n",
      "68.hamilton.txt\n",
      "    president: 16.4398\n",
      "    senator: 10.0321\n",
      "    votes: 9.7887\n",
      "    vote: 8.1790\n",
      "    electors: 7.8310\n"
     ]
    }
   ],
   "source": [
    "# Example using tf-df to get most important words from all\n",
    "# the federalist papers\n",
    "\n",
    "main('data/federalist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction\n",
    "The task of extracting knowledge from documents\n",
    "\n",
    "### Note: their were examples for basic information extraction in the lexture at 1:25, but the source code supplied did not work, so I deleted it. \n",
    "\n",
    "### WordNet\n",
    "A built-in part of nltk that is basically a dictionary for many known words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: city\n",
      "\n",
      "city.n.01: a large and densely populated urban area; may include several independent administrative districts\n",
      "  municipality.n.01\n",
      "\n",
      "city.n.02: an incorporated administrative district established by state charter\n",
      "  administrative_district.n.01\n",
      "\n",
      "city.n.03: people living in a large densely populated municipality\n",
      "  municipality.n.02\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "word = input(\"Word: \")\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "for synset in synsets:\n",
    "    print()\n",
    "    print(f\"{synset.name()}: {synset.definition()}\")\n",
    "    for hypernym in synset.hypernyms():\n",
    "        print(f\"  {hypernym.name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Representation\n",
    "Ideally we want an ai to be able to take a word in a given sentence and understand what that word means, one of the easiest way for a computer to do so is by converting words to numbered vectors.\n",
    "\n",
    "\n",
    "**One-Hot Representation** - A representation of meaning as a vector with a single 1, and with other values as 0, the above word assignments for 'He wrote a book.' is an example of this. This type of word representation will only really work for smaller word banks. Also another issue is that all words will have different vectors, even words with the same meaning like 'book' and 'novel', see example below:\n",
    "\n",
    "* Initial Sentence: 'He wrote a book.'\n",
    "* he = [1, 0, 0, 0]\n",
    "* wrote = [0, 1, 0, 0]\n",
    "* a = [0, 0, 1, 0]\n",
    "* book = [0, 0, 0, 1]\n",
    "\n",
    "**Distribution Representation** - A representation of meaning distributed across multiple numerical values. With this type of representation, similar words should have vectors with close to the same values, see example below:\n",
    "\n",
    "* Initial Sentence: 'He wrote a book.'\n",
    "* he = [-0.34, -0.08, 0.02, -0.18, ...]\n",
    "* wrote = [-0.27, 0.40, 0.00, -0.65, ...]\n",
    "* a = [-0.12, -0.25, 0.29, -0.09, ...]\n",
    "* book = [-0.23, -0.16, -0.05, -0.57]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## word2vec\n",
    "A model for generating word vectors\n",
    "\n",
    "\n",
    "**Skip-Gram Architecture** - Neural network architecture for predicting context words given a target word. In such a neural network there is usually 1 hidden node layer that assigns numerical values to each input word (every word is an input node). The hidden layer node values will actually be the vector representation of that word (see distribution representation above). So if a hidden layer with 4 nodes is created, every target word will get it's own vector representation with 4 numerical values. Words with similar meanings should have vector values that are close together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.231087 -0.238098  0.584713 -0.524351  0.40278   0.148448  0.386096\n",
      " -0.493994 -0.198922 -0.411161  0.556962  0.220978 -0.304637 -0.499713\n",
      " -0.092555  0.262613  0.752704  0.463667  0.054477  0.155809 -0.195134\n",
      " -0.009269  0.378139 -0.651306 -0.029372 -0.563472  0.024709  0.366842\n",
      " -0.476904 -0.42565  -0.094642 -0.052822  0.124612  0.296046 -0.244881\n",
      "  0.195957  0.223666  0.064116  0.577874  0.083096 -0.378262  0.196044\n",
      " -0.220993 -0.630213 -0.311214  0.435611  0.351486  0.342794 -0.229961\n",
      " -0.157521  0.204315  0.253944 -0.562277  0.534482 -0.4158    0.120161\n",
      "  0.649395 -0.227012 -0.130488 -0.332326  0.691952 -0.400436  0.410125\n",
      "  0.026237 -0.408483  0.188236  0.130957 -0.320686  0.225932 -0.171665\n",
      " -0.335107 -0.009982  0.680831 -0.023788 -0.165798  0.345986 -0.232295\n",
      "  0.021137  0.08515  -0.24387  -0.142469 -0.058325  0.086046 -0.173068\n",
      "  0.198108  0.009103  0.381725  0.095911  0.317972 -0.10012   0.143178\n",
      "  0.106724 -0.419844 -0.175785 -0.251805  0.211927  0.411175  0.317378\n",
      "  0.450316 -0.252661]\n",
      "\n",
      "Distance \"book\" is from itself:\n",
      "0.0\n",
      "\n",
      "Distance \"book\" is from the word \"breakfast\":\n",
      "0.6351827719357863\n",
      "\n",
      "Distance \"book\" is from the word \"novel\":\n",
      "0.3436623421719047\n",
      "\n",
      "10 closest words to \"book\"\n",
      "['book', 'books', 'essay', 'memoir', 'essays', 'novella', 'anthology', 'blurb', 'autobiography', 'audiobook']\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/words.txt\") as f:\n",
    "    words = dict()\n",
    "    for i in range(50000):\n",
    "        row = next(f).split()\n",
    "        word = row[0]\n",
    "        vector = np.array([float(x) for x in row[1:]])\n",
    "        words[word] = vector\n",
    "\n",
    "\n",
    "def distance(w1, w2):\n",
    "    return cosine(w1, w2)\n",
    "\n",
    "\n",
    "def closest_words(embedding):\n",
    "    distances = {\n",
    "        w: distance(embedding, words[w])\n",
    "        for w in words\n",
    "    }\n",
    "    return sorted(distances, key=lambda w: distances[w])[:10]\n",
    "\n",
    "\n",
    "def closest_word(embedding):\n",
    "    return closest_words(embedding)[0]\n",
    "\n",
    "# This is the vector array that represents the word 'city'\n",
    "print(words['city'])\n",
    "\n",
    "print()\n",
    "\n",
    "print('Distance \"book\" is from itself:')\n",
    "print(distance(words['book'], words['book']))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Distance \"book\" is from the word \"breakfast\":')\n",
    "print(distance(words['book'], words['breakfast']))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Distance \"book\" is from the word \"novel\":')\n",
    "print(distance(words['book'], words['novel']))\n",
    "\n",
    "print()\n",
    "print('10 closest words to \"book\"')\n",
    "print(closest_words(words['book'])[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because word2vec utilizes numerical vectors for word representation, mathematicaly formulas can be used to calculate words. See example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_word(words['king'] - words['man'] + words['woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_word(words['paris'] - words['france'] + words['england'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
