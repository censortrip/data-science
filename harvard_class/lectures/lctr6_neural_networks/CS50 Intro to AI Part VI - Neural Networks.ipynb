{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Artificial Intelligence with Python\n",
    "\n",
    "## Part VI - Neural Networks\n",
    "\n",
    "Harvard CS50 Introduction to Artificial Intelligence with Python is an online course that I took in the Spring of 2020. It consisted of 6 lectures of which I have a notebook for each. Each lecture consisted of accompanying projects which are located in the projects folder in the same directory as this notebook.\n",
    "\n",
    "[Course Link](https://cs50.harvard.edu/ai/)\n",
    "\n",
    "[Lecture Link](https://www.youtube.com/watch?v=mFZazxxCKbw&list=PLhQjrBD2T382Nz7z1AEXmioc27axa19Kv&index=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "Mathematical model for learning inspried by biological neural networks.\n",
    "* Artificial neural networks model mathematical function from inputs to outputs based on the structure and paramters of the network \n",
    "* allows for learning the network's parameters based on data\n",
    "* Rather than real nueurons from a biological network, ai neural networks use units (similar to the idea of a node) that represent artificial neurons, that connect to one another in a network. These units can pass along inputs and outputs\n",
    "\n",
    "Recall from the ML lecture that a hypothesis function for a machine learning task took in inputs and produced outputs based on pre-determined weights like so:\n",
    "\n",
    "* h(x1, x2) = w0 + w1 * x1 + w2 * x2\n",
    "\n",
    "With machine learning, we saw that many of the algorithms of ML passed the hypothesis calculation results through an 'activation function' like so:\n",
    "\n",
    "* h(x1, x2) = g(w0 + w1 * x1 + w2 * x2)\n",
    "* the w0 is known as the bias value and is not assigned to any input\n",
    "\n",
    "Some activation function are (note, some in lecture 5 notes):\n",
    "* Step Functions: g(x) = 1 if x >= 0, else 0\n",
    "* Logistic Sigmoid (s-shape curve) Functions: g(x) = e^x / e^x + 1\n",
    "* Rectified Linear Unit (ReLU): g(x) = max(0, x)\n",
    "\n",
    "To sum up, activiation functions get applied to the results of a hypothesis function and usually the results are some linear combination (rain, no rain) type of problems. \n",
    "\n",
    "The above example of a hypothesis with two inputs that have a solution passed through an activation function can represent the simplest of neural networks, where the two left units representing the inputs are connected to one output like so:\n",
    "\n",
    "<img src='images/nn2.png'>\n",
    "\n",
    "Note in the above image that the two inputs are connected to the output and each connection is weighted which is used gy the output g(x) function to calculate the results.\n",
    "\n",
    "---\n",
    "\n",
    "### Example of Simple Neural Network \n",
    "This example trains a neural network to be able to learn a specific function, in this case the Or function as pictured below (0 if False, 1 is True):\n",
    "\n",
    "<img src='images/nn3.png'>\n",
    "\n",
    "Using the simple network idea above, weights (1) and bias (-1) can be set in order to model the Or function like so:\n",
    "\n",
    "<img src='images/nn5.png'>\n",
    "\n",
    "In the above example, only 0 or 1 are possible choices and they are chosen by whether or not they cross the threshold (the dotted line or at 0).\n",
    "\n",
    "In the below image:\n",
    "* x1=0, therefore (x1 * w1) = (0 * 1) = 0\n",
    "* x2=0, therefore (x2 * w2) = (0 * 1) = 0\n",
    "* the total calculation is g(-1 + 0 + 0) or -1\n",
    "* therefore the overall results is 0 (or False) as -1 is before the threshold of 0\n",
    "\n",
    "<img src='images/nn8.png'>\n",
    "\n",
    "One more example:\n",
    "* x1=1, therefore (x1 * w1) = (1 * 1) = 1\n",
    "* x2=0, therefore (x2 * w2) = (0 * 1) = 0\n",
    "* the total calculation is g(-1 + 1 + 0) or 0\n",
    "* therefore the overall results is 1 (or True) as 0 is the threshold\n",
    "\n",
    "<img src='images/nn7.png'>\n",
    "\n",
    "Using a simple model like above, some of the examples we already used in previous lectures can also be modeled like humidty, pressure determining rain or advertising dollars spent to sales\n",
    "\n",
    "<img src='images/nn9.png'>\n",
    "\n",
    "In all the previous examples, only two inputs were used, but multiple inputs can be added as well, the below images shows 5 inputs mapped to one output:\n",
    "\n",
    "<img src='images/nn10.png'>\n",
    "\n",
    "---\n",
    "\n",
    "## Training Neural Networks (Determining  Ideal Weights)\n",
    "\n",
    "**Gradient Descent** - Algorithm for minimizing loss(how off the hypothesis function is) when training a neural network\n",
    "\n",
    "High level implementation of gradient descent with a neural network:\n",
    "* Start with a random choice of weights\n",
    "* Repeat: \n",
    "    * Calculate the gradient based on all data points direction that will lead to decreasing loss\n",
    "    * Update weights according to the gradient\n",
    "\n",
    "Because this algorithm makes calculations based on ALL DATA POINTS it is non-efficient for sufficiently large data sets.\n",
    "\n",
    "Some more efficient alternatives to gradient descent:\n",
    "\n",
    "**Stochastic Gradient Descent** - same setup as regular gradient decent except it just randomly chooses ONE data point at a time to perform weight calculations on rather than all the data points at once\n",
    "\n",
    "**Mini-Batch Gradient Descent** -  same setup as regular gradient except it calculates based on small groups of data points or 'batches'\n",
    "\n",
    "\n",
    "Neural Networks can have multple output layers just like they do for inputs and the only real difference to the calculations is that more weights have to be added (to match the number of outputs, so in this example each input would have four separate weights, one for each output, like so:\n",
    "\n",
    "<img src='images/nn11.png'>\n",
    "\n",
    "An example of muliple outputs would be with weather where the outcomes could represent the probabilty of each particular weather event. So the below diagram represents that based on the three inputs the model predicts that there is a 10% chance of rain, 60% chance of sun, and so on: \n",
    "\n",
    "<img src='images/nn12.png'>\n",
    "\n",
    "The above networks allow for more complicated classifications more than just binary (rain/no rain) performed previously. Such networks can also be used with reinforcement learning to model ideal actions to take like so: \n",
    "\n",
    "<img src='images/nn13.png'>\n",
    "\n",
    "The overall message here is that neural networks are broadly applicable to many different machine learning algorithms and problem types. \n",
    "\n",
    "To determine weights for multi-output neural networks, each output and inputs can be treated as separate neural networks, note in the image below, the three inputs have separate weights for each output, yet they all three have a mapping to each output, therefore each output and its particular weight mappings to the inputs can be viewed separately from the rest and calculated as individual neural networks\n",
    "\n",
    "<img src='images/nn14.png'>\n",
    "\n",
    "**Most of the above examples fall under the category of a Perceptron (recall that this is a binary classifier model, see lecture 5 notes for more info), which as a downside:**\n",
    "* are only capable of learning linearly seprable decision boundries (data that can be split evenly with regression line), and therefore cannot calculate more complex data like the circle example from lecture 5 where a straight line would not work.\n",
    "\n",
    "\n",
    "---\n",
    "## Multilayer Neural Network\n",
    "Artificial Neural Network with an input layer, an output layer, and at least one hidden layer (in the image below, the center layer is the hidden one). There can be multiple hidden layers inside of a network. Note that in multilayer networks, the input layer are not directly connected to the output layer but rather to the hidden layer. Weights are still handled exactly the same way between inputs, hidden layers, and output. \n",
    "\n",
    "<img src='images/nn15.png'>\n",
    "\n",
    "---\n",
    "### Training Multilayer Neural Network\n",
    "\n",
    "**Backpropagation** - algorithm for training multilayer neural networks (or networks with hidden layers), the main algorithm that makes neural networks possible.\n",
    "\n",
    "High level implementation of Backpropagation with a multilayer neural network:\n",
    "* Start with a random choice of weights\n",
    "* Repeat:\n",
    "    * Calculate error for output layer\n",
    "    * For each layer, starting with output layer, and moving inwards towards earliest hidden layer:\n",
    "        * Propagate error back one layer\n",
    "        * Update weights\n",
    "\n",
    "\n",
    "**Deep Neural Network** - Neural Network with multiple hidden layers, see image below:\n",
    "\n",
    "<img src='images/nn16.png'>\n",
    "\n",
    "**Overfitting** - can occur in a deep neural network when too many hidden layers are created that prevent the input data from generalizing well to the output\n",
    "\n",
    "**Dropout** - temporarily removing units (selected randomly) from a nerual network to prevent over-reliance on certain units, this is handled similarly to the epsilon greedy reinforcement algorithm from lecture 5 and project 5a 'Nim', where psuedo-random actions were made in order to train a game so that the ai didn't make the same move over and over. \n",
    "\n",
    "To see the dropout technique in action, a multilayer network may be trained by randomly dropping out some units like so:\n",
    "\n",
    "<img src='images/nn17.png'>\n",
    "\n",
    "At this point, once the weights that were trained are updated, then another random selection could be made like so and then after the weights would be re-updated and so on and so forth:\n",
    "\n",
    "<img src='images/nn18.png'>\n",
    "\n",
    "---\n",
    "## Implementing Neural Networks in Code using TensorFlow\n",
    "One of the more well known libraries for neural networks is TensorFlow (created by Google). \n",
    "\n",
    "[Tensorflow Playground Link](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=&seed=0.68443&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 823 samples\n",
      "Epoch 1/20\n",
      "823/823 [==============================] - 0s 499us/sample - loss: 1.6509 - accuracy: 0.3803\n",
      "Epoch 2/20\n",
      "823/823 [==============================] - 0s 63us/sample - loss: 1.3413 - accuracy: 0.3597\n",
      "Epoch 3/20\n",
      "823/823 [==============================] - 0s 68us/sample - loss: 1.0729 - accuracy: 0.3244\n",
      "Epoch 4/20\n",
      "823/823 [==============================] - 0s 66us/sample - loss: 0.8736 - accuracy: 0.3123\n",
      "Epoch 5/20\n",
      "823/823 [==============================] - 0s 67us/sample - loss: 0.7307 - accuracy: 0.4192\n",
      "Epoch 6/20\n",
      "823/823 [==============================] - 0s 64us/sample - loss: 0.6330 - accuracy: 0.6185\n",
      "Epoch 7/20\n",
      "823/823 [==============================] - 0s 70us/sample - loss: 0.5604 - accuracy: 0.7217\n",
      "Epoch 8/20\n",
      "823/823 [==============================] - 0s 63us/sample - loss: 0.5048 - accuracy: 0.7849\n",
      "Epoch 9/20\n",
      "823/823 [==============================] - 0s 65us/sample - loss: 0.4590 - accuracy: 0.8299\n",
      "Epoch 10/20\n",
      "823/823 [==============================] - 0s 69us/sample - loss: 0.4202 - accuracy: 0.8457\n",
      "Epoch 11/20\n",
      "823/823 [==============================] - 0s 68us/sample - loss: 0.3865 - accuracy: 0.8615\n",
      "Epoch 12/20\n",
      "823/823 [==============================] - 0s 61us/sample - loss: 0.3563 - accuracy: 0.8712\n",
      "Epoch 13/20\n",
      "823/823 [==============================] - 0s 69us/sample - loss: 0.3297 - accuracy: 0.8870\n",
      "Epoch 14/20\n",
      "823/823 [==============================] - 0s 71us/sample - loss: 0.3057 - accuracy: 0.8955\n",
      "Epoch 15/20\n",
      "823/823 [==============================] - 0s 57us/sample - loss: 0.2843 - accuracy: 0.9077\n",
      "Epoch 16/20\n",
      "823/823 [==============================] - 0s 65us/sample - loss: 0.2650 - accuracy: 0.9149\n",
      "Epoch 17/20\n",
      "823/823 [==============================] - 0s 76us/sample - loss: 0.2474 - accuracy: 0.9235\n",
      "Epoch 18/20\n",
      "823/823 [==============================] - 0s 71us/sample - loss: 0.2318 - accuracy: 0.9271\n",
      "Epoch 19/20\n",
      "823/823 [==============================] - 0s 63us/sample - loss: 0.2170 - accuracy: 0.9320\n",
      "Epoch 20/20\n",
      "823/823 [==============================] - 0s 64us/sample - loss: 0.2036 - accuracy: 0.9368\n",
      "549/549 - 0s - loss: 0.1996 - accuracy: 0.9417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19956535589499552, 0.9417122]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a Tensorflow Neural Network That will\n",
    "# will help identify authentic and counterfeit banknotes \n",
    "# This code is discussed 45 minutes into lecture\n",
    "\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read data in from file\n",
    "with open(\"banknotes.csv\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append({\n",
    "            \"evidence\": [float(cell) for cell in row[:4]],\n",
    "            \"label\": 1 if row[4] == \"0\" else 0\n",
    "        })\n",
    "\n",
    "# Separate data into training and testing groups\n",
    "evidence = [row[\"evidence\"] for row in data]\n",
    "labels = [row[\"label\"] for row in data]\n",
    "X_training, X_testing, y_training, y_testing = train_test_split(\n",
    "    evidence, labels, test_size=0.4\n",
    ")\n",
    "\n",
    "# Create a neural network\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 8 units, with ReLU activation\n",
    "# Dense means that each hidden layer node will be connected to a previous node\n",
    "# note activation function is the g(x) from initial basic example\n",
    "model.add(tf.keras.layers.Dense(8, input_shape=(4,), activation=\"relu\"))\n",
    "\n",
    "# Add output layer with 1 unit, with sigmoid activation\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(X_training, y_training, epochs=20)\n",
    "\n",
    "# Evaluate how well model performs\n",
    "model.evaluate(X_testing, y_testing, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Some Real-World Applications for Deep Neural Networks\n",
    "\n",
    "\n",
    "### Computer Vision\n",
    "computational methods for analyzing and understanding digital images\n",
    "\n",
    "**Image Convolution** - applying a filter (matrix) that adds each pixel value of an image to its neighbors, weighted according to a kernal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "# Open image\n",
    "image = Image.open('images/bridge.png').convert(\"RGB\")\n",
    "\n",
    "# Filter image according to edge detection kernel\n",
    "filtered = image.filter(ImageFilter.Kernel(\n",
    "    size=(3, 3),\n",
    "    kernel=[-1, -1, -1, -1, 8, -1, -1, -1, -1],\n",
    "    scale=1\n",
    "))\n",
    "\n",
    "# Show resulting image\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling** - reducing the size of an input by sampling from regions in the input (for example, only max pixels from certain quadrants)\n",
    "\n",
    "### Convolutional Neural Network\n",
    "Nerual Networks that use convolution, usually for analyzing images. This type of model combines convolution and pooling in phases in order to learn various features of an image. \n",
    "* Rather than start by taking a base image and putting all of its inputs into a hidden layer, this model starts with a convolution step, which involves applying filters to an image to get some resulting **feature map**\n",
    "* Convolution is usually applied a number of times to get multiple feature maps, each of which might extract some imporant characterstic from an image\n",
    "* After convolution is applied and feature maps are discovered, pooling is performed to reduce the size of the feature maps\n",
    "* After pooling is complete and overall image features have been condensend, flattening can occur and then these results can be put into a neural network as inputs for each pixel value from each feature map. \n",
    "\n",
    "The image below visually depicts the above process for taking an image, applying convolution to get resulting feature maps, then applying pooling to the feature maps to condense them, then flattening the resulting condensed maps in order to use them as inputs for a neural network (note the first square on the left represents the initial image and all its pixels):\n",
    "\n",
    "<img src='images/nn19.png'>\n",
    "\n",
    "In many cases in a convolutional neural network, the above process is performed more than once in order to extract even more precise features. In the image below, the first round of convolution and pooling extracts low-level featurs of an image like its edges, curves, and shapes, then after a second round of convolution and pooling higher-level features like actual objects can be extracted:\n",
    "\n",
    "<img src='images/nn20.png'>\n",
    "\n",
    "### Convolution Neural Network Used for Handwriting Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 31s 517us/sample - loss: 0.2571 - accuracy: 0.9225\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 31s 524us/sample - loss: 0.1068 - accuracy: 0.9680\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 31s 517us/sample - loss: 0.0818 - accuracy: 0.9745\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 526us/sample - loss: 0.0676 - accuracy: 0.9793\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 32s 528us/sample - loss: 0.0539 - accuracy: 0.9827\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 32s 527us/sample - loss: 0.0473 - accuracy: 0.9854\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 31s 513us/sample - loss: 0.0423 - accuracy: 0.9861\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 30s 502us/sample - loss: 0.0359 - accuracy: 0.9882\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 30s 498us/sample - loss: 0.0320 - accuracy: 0.9893\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 30s 492us/sample - loss: 0.0278 - accuracy: 0.9911\n",
      "10000/10000 - 2s - loss: 0.0389 - accuracy: 0.9874\n",
      "Model saved to model.h5.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use MNIST handwriting dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Prepare data for training\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "x_train = x_train.reshape(\n",
    "    x_train.shape[0], x_train.shape[1], x_train.shape[2], 1\n",
    ")\n",
    "x_test = x_test.reshape(\n",
    "    x_test.shape[0], x_test.shape[1], x_test.shape[2], 1\n",
    ")\n",
    "\n",
    "# Create a convolutional neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "\n",
    "    # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
    "    tf.keras.layers.Conv2D(\n",
    "        32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)\n",
    "    ),\n",
    "\n",
    "    # Max-pooling layer, using 2x2 pool size\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    # Flatten units\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Add a hidden layer with dropout\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "    # Add an output layer with output units for all 10 digits\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# Train neural network\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "# Evaluate neural network performance\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "# Save model to file (so can be reused without needing to train again)\n",
    "filename = 'model.h5'\n",
    "model.save(filename)\n",
    "print(f\"Model saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above code created and trained a convolution neural network to understand various digits. After training was complete, the model was saved into a file called model.h5. This file can be used to actually run the model in a real setting. See recongnition.py to run this program** \n",
    "\n",
    "---\n",
    "\n",
    "## Feed-Forward Neural Network (1 to 1 neural network)\n",
    "A feedforward neural network is an artificial neural network wherein connections between the nodes do not form a cycle. As such, it is different from recurrent neural networks. The feedforward neural network was the first and simplest type of artificial neural network devised\n",
    "\n",
    "A FFNN has connections only in one direction, from one layer to another and they only put in one vector of inputs and get one vector of outputs.\n",
    "\n",
    "<img src='images/nn22.png'>\n",
    "\n",
    "## Recurrent Neural Network (1 to many neural network)\n",
    "A recurrent neural network is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state to process variable length sequences of inputs\n",
    "\n",
    "RNN's are particulalry useful when dealing with sequences of data. (example, videos which are sequences of images)\n",
    "\n",
    "<img src='images/nn21.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
