{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'target' variable) and one or more independent variables (often called 'predictors', 'covariates', or 'features'). The most common form of regression analysis is linear regression.\n",
    "\n",
    "Regression analysis is primarily used for two conceptually distinct purposes:\n",
    "1. Prediction and Forecasting\n",
    "2. In some situations regression analysis can be used to infer causal relationships between the independent and dependent variables\n",
    "\n",
    "regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. \n",
    "\n",
    "To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using observational data.\n",
    "\n",
    "This notebook goes over many of the most common ML regression models including:\n",
    "\n",
    "1. [Linear Regression (Simple & Multiple)](#one)\n",
    "2. [Ridge, Lasso, and Elastic-Net Regression](#two)\n",
    "3. [Polynomial Regression](#three)\n",
    "4. [Generalized Linear Models](#four)\n",
    "5. [Bayesian Regression](#five)\n",
    "6. [Least Angle Regression (LARS)](#six)\n",
    "7. [Support Vector Machine Regression (SVM)](#seven)\n",
    "8. [Stochastic Gradient Descent Regression (SGD)](#eight)\n",
    "9. [Nearest Neighbor Regression](#nine)\n",
    "10. [Guassian Process Regression (GPR)](#ten)\n",
    "11. [Decision Tree Regression](#eleven)\n",
    "12. [Random Forest Regression](#twelve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='one'></a>1. Linear Regression (Simple & Multiple)\n",
    "**Linear Regression** is a **supervised linear model** that establishes a relationship between a dependent variable (y) and one or more independent variables (X) using a best fit straight line (also known as the regression line). Machine learning linear regression models attempt to determine the best fit line formula (y = b + mX) by calculating the ideal slope (m) and intercept (b). \n",
    "\n",
    "Note that the **best fit line** is a straight line attempting to represent all of the data points in a data set as accurately as possible. If a data set is not linear in nature, i.e. curved, then a simple linear model will not be able to captue the true relationship between the data with a straight line. \n",
    "\n",
    "---\n",
    "\n",
    "## Types of Linear Regression:\n",
    "\n",
    "1. **Simple Linear Regression** - a linear regression model with only 1 explanatory variable (X)\n",
    "2. **Multiple Linear Regression** - a linear regression model with >1 explanatory (X) variables\n",
    "\n",
    "---\n",
    "\n",
    "## Linear Regression Formula: y = b + mX\n",
    "* Where, y = dependent variable, X = independent variable, b = y-intercept, m = slope of the line (or coefficient)\n",
    "\n",
    "The most common method for obtaining the 'best fit line' is to use the **Least Squares Method**. For the math behind this method see the link below:\n",
    "\n",
    "https://www.mathsisfun.com/data/least-squares-regression.html\n",
    "\n",
    "---\n",
    "\n",
    "## Common use-cases for linear regression are:\n",
    "\n",
    "* **Business trend analysis and forecasting** (example, company trying to predict sales numbers based on amount spent on advertising or estimating growth in sales future sales based on current economic conditions)\n",
    "* **Time Series Modeling** (stock prices)\n",
    "* **Pricing change impact on business**\n",
    "* **Risk assesments** (insurance, financial services, etc.)\n",
    "* **Numerical weather predictions** (what tempurature tomorrow)\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions of Linear Regression\n",
    "There are 4 major assumptions that linear regression models have. Note, much of this info came from this site:\n",
    "\n",
    "https://www.statology.org/linear-regression-assumptions/\n",
    "\n",
    "### Assumption 1 - Linear Relationship\n",
    "The first assumption of linear regression is that there is a linear relationship between the independent variable (X) and the independent variable (y). This also holds for the variables with the output in multiple linear regression. \n",
    "\n",
    "### How to determine if this assumption is met:\n",
    "* **Scatter Plots** between the two variables\n",
    "\n",
    "* **Correlation Coefficient (r)** - describes the degree of strength of a regression line between **ONLY** one X (independent variable) and one y (dependent variable). The values of the correlation coefficient can be between 0 and 1, where 0 = no correlation at all, and 1 = perfect correlation.\n",
    " \n",
    "* **Coefficient of Determination (r^2)**  - describes the change in y by the change in X. This metric is a relative measure of the % of the dependent (y) variable variance that the model explains. Can range from 0 to 100%. \n",
    "    * Note  R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots. For example: you can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data. R-squared will always increase as you add more features to a model, even if they are unrelated to the target variable, therefore simply selecting a model with the highest r-squared is not a reliable apporach for choosing the best linear model, solutions to this are using train/test/split or cross-validation.\n",
    "    \n",
    "**A simple example of r and r^2 and their interpretations:**\n",
    "* r = 0.94, means that the regression line has a strong positive linear 'fit' to the data because close to 1\n",
    "* r^2 = 0.94^2 = 0.884, means that 88.4% of the change in y can be explained in the change in X\n",
    "\n",
    "**Below is a diagram visually depicting various correlation coefficient (r) values:**\n",
    "<img src='images/cc.png'>\n",
    "\n",
    "### What to do if this assumption is violated:\n",
    "* **Option 1:** apply a nonlinear transformation to the independent and/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and/or dependent variable.\n",
    "\n",
    "* **Option 2:** add another independent variable to the model. For example, if the plot of x vs. y has a parabolic shape then it might make sense to add X2 as an additional independent variable in the model.\n",
    "\n",
    "**IMPORTANT** One potential issue with **multiple linear regression models** can be multicollinearity:\n",
    "\n",
    "**Multicollinearity** - (aka collinearity) generally occurs when there are high correlations between two or more predictor/feature variables. In other words, one predictor variable can be used to predict the other. This creates redundant information, skewing the results in a regression model. Examples of correlated predictor variables (also called multicollinear predictors) are: a person’s height and weight, age and sales price of a car, or years of education and annual income. Multicollinearity is most commonly found in observational studies rather than with experimental data. Multicollinearity can be detected in a few ways:\n",
    "* Calculate **correlation coefficients** for all pairs of predictor variables. If the correlation coefficient, r, is exactly +1 or -1, this is called perfect multicollinearity. If r is close to or exactly -1 or +1, one of the variables should be removed from the model if at all possible.\n",
    "* Another statistical method for detecting multicollinearity between varibales is the **Variance Inflation Factor (VIF)** which takes each dependent (X) feature and regresses it against all of the other features. A VIF < 2.5 good, anything higher  5 is cause for concern, and anything > 10 is a serious collinearity problem. \n",
    "\n",
    "Some causes of multicollinearity are:\n",
    "* Insufficient data amount\n",
    "* Dummy variables being incorrectly used (ex: failing to exclude one category, or adding a dummy variable for every category (spring, summer, fall, winter)\n",
    "* Including a variable that is a combo of two other variables\n",
    "* Including two identical (or nearly identical) variables (ex: weight in pounds and weight in kilos)\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 2 Independence (No Autocorrelation)\n",
    "The next assumption of linear regression is that the erros/residuals are independent. This is mostly relevant when working with time series data. Ideally, we don’t want there to be a pattern among consecutive errors. For example, residuals shouldn’t steadily grow larger as time goes on (Autocorrelation).\n",
    "\n",
    "**Autocorrelation (aka Serial Correlation)** - is where error terms in a time series transfer from one period to another. In other words, the error for one time period a is correlated with the error for a subsequent time period b. For example, an underestimate for one quarter’s profits can result in an underestimate of profits for subsequent quarters. This can result in a myriad of problems, including:\n",
    "\n",
    "* Inefficient ordinary least squares model predictions\n",
    "* Exaggerated regression line goodness of fit\n",
    "* False positives for significant regression coefficients (i.e. the coefficient appears to be statistically significant when it's not)\n",
    "\n",
    "### How to determine if this assumption is met:\n",
    "* **Option 1:** is to create a residual time series plot, which is a plot of residuals vs. time. \n",
    "\n",
    "* **Option 2: Durbin-Watson Test** is used to detect the presence of autocorrelation at lag 1 in the residuals (prediction errors) from a regression analysis. If the test results fall between 1.5 and 2.5, no autocorellation exsits in the data\n",
    "\n",
    "### What to do if this assumption is violated:\n",
    "* **Option 1:** For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model\n",
    "\n",
    "* **Option 2:** For negative serial correlation, check to make sure that none of your variables are overdifferenced.\n",
    "    \n",
    "* **Option 3:** For seasonal correlation, consider adding seasonal dummy variables to the model.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 3 Homoscedasticity of errors (or equal variance around the line)\n",
    "The next assumption of linear regression is that the residuals have constant variance at every level of x. This is known as homoscedasticity.  When this is not the case, the residuals are said to suffer from heteroscedasticity.\n",
    "\n",
    "**Heteroscedasticity** - in statistics, heteroscedasticity happens when the standard errors of a variable, monitored over a specific amount of time, are non-constant. With heteroscedasticity, the tell-tale sign upon visual inspection of the residual errors is that they will tend to fan out over time in a cone shape. Note that while heteroscedasticity does not cause bias in the coefficient estimates, it does make them less precise. Scatter plots can be used to visually see if the data has this condition. See the image below for a visual example:\n",
    "<img src='images/hetero.png'>\n",
    "    \n",
    "This link provides a good overview of heteroskedasticity: \n",
    "https://www.investopedia.com/terms/h/heteroskedasticity.asp\n",
    "\n",
    "When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.\n",
    "\n",
    "### How to determine if this assumption is met:\n",
    "\n",
    "The simplest way to detect heteroscedasticity is by creating a fitted value vs. error/residual scatter plot\n",
    "\n",
    "<img src='images/hetero2.png'>\n",
    "\n",
    "### What to do if this assumption is violated:\n",
    "* **Option 1: Transform the dependent variable** - One common transformation is to simply take the log of the dependent variable. For example, if we are using population size (independent variable) to predict the number of flower shops in a city (dependent variable), we may instead try to use population size to predict the log of the number of flower shops in a city. Using the log of the dependent variable, rather than the original dependent variable, often causes heteroskedasticity to go away.\n",
    "* **Option 2: Redefine the dependent variable** - One common way to redefine the dependent variable is to use a rate, rather than the raw value. For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita. In most cases, this reduces the variability that naturally occurs among larger populations since we’re measuring the number of flower shops per person, rather than the sheer amount of flower shops.\n",
    "* **Option 3: Use weighted regression** - Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumption 4 - Errors are Normally Distributed\n",
    "The next assumption of linear regression is that the **errors (aka residuals)** are normally distributed, note that error values are ('test_y' - 'predicted_y'). \n",
    "\n",
    "**Important Note 1:**\n",
    "Assumption 4 is not necessary, however without this assumption being satisfied, one cannot calculate 'confidence' or 'prediction' intervals. Also note that in real-world scenarios, risiduals are rarely completely normal. \n",
    "\n",
    "**Important Note 2:**\n",
    "With Linear Regression, individuals features and target variables DO NOT have to have normal distributions unto themselves. Note that even though normality with individual variables is not required, taking a look at them can be beneficial as:\n",
    "* Highly skewed variables can, more than likely, influence the distribution of residuals making them, in turn, non-normal\n",
    "* Variables with very large tails (containing outliers) could require a complex analysis of leverage (i.e. how much these outliers impact on the estimate of the regression coefficients)\n",
    "\n",
    "Therefore for very skewed variables, transforming the data (to make normal) can be a good idea to avoid any potential harmful effect.\n",
    "\n",
    "\n",
    "### How to determine if this assumption is met:\n",
    "* **QQ-Plots (probplot in scipy)** can be used to check for normality. Note that the **error values should be graphed**, NOT the actual or test values. \n",
    "\n",
    "### What to do if this assumption is violated:\n",
    "* **Step 1:** Verify that any outliers aren’t having a huge impact on the distribution. If there are outliers present, make sure that they are real values and that they aren’t data entry errors. Note that linear regresssion is **VERY sensitive to outliers**\n",
    "\n",
    "* **Step 2:** Next, you can apply a nonlinear transformation to the independent and/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and/or dependent variable. **NOTE THAT THIS STEP MAY HAVE ALREADY BEEN TAKEN CARE OF DURING ASSUMPTION 1**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Selecting the Most Significant Indepent Variables in the case of Multiple Linear Regression\n",
    "\n",
    "In the case of multiple independent variables, forward selection and backward elimination can be used to select the most siginificant independent variables:\n",
    "\n",
    "**Forward Selection** Forward selection is a type of stepwise regression which begins with an empty model and adds in variables one by one. In each forward step, you add the one variable that gives the single best improvement to your model.\n",
    "\n",
    "**Backward Elimination**  is almost the opposite of forward selection. With backward elimination you start with a model that includes every possible variable and eliminate the extraneous variables one by one.\n",
    "\n",
    "---\n",
    "\n",
    "## Linear Regression Model Evaluation Metrics\n",
    "There a few methodologies for evaluating linear regression models. **In all cases the lower the value the better**, however the scale will differ based on the original data:\n",
    "\n",
    "**Mean Absolute Error** - Absolute error is the amount of error from individual dependent (y) outcome variables and the actual (X) test variables. Mean absolute error is the average of all the absolute errors.\n",
    "\n",
    "**Mean Squared Error** - tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. The squaring is necessary to remove any negative signs. It also gives more weight to larger differences. It’s called the mean squared error as you’re finding the average of a set of errors.\n",
    "\n",
    "**Root Mean Squared Error** is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.\n",
    "\n",
    "### Evaluation Formulers\n",
    "Note that y = original test values, y-bar = predicted y values\n",
    "\n",
    "<img src='images/slr_eval.png'>\n",
    "\n",
    "### When  and how to use these metrics:\n",
    "The MSE can be useful but is not very intuitive as it is not in scale with the original data metrics. \n",
    "\n",
    "RMSE and MAE are similar as both are in scale to the origina data metric units making them easier to interpret than MSE. \n",
    "\n",
    "**MAE is conceptually simpler and more interpretable than RMSE**. MAE does not require the use of squares or square roots. The use of squared distances hinders the interpretation of RMSE. **MAE is simply the average absolute vertical or horizontal distance between each point in a scatter plot and the Y=X line**. \n",
    "\n",
    "In other words, **MAE is the average absolute difference between X and Y**. MAE is fundamentally easier to understand than the square root of the average of the sum of squared deviations. Furthermore, each error contributes to MAE in proportion to the absolute value of the error, which is not true for RMSE; **because RMSE involves squaring the difference between the X and Y, a few large differences (outliers) will increase the RMSE to a greater degree than the MAE**, thus **RMSE is more sensitive to outliers than MAE*\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Linear Regression By Hand\n",
    "Below I create two variables both with normal distributed values and run a simple linear regression between them by hand. \n",
    "\n",
    "This site has a really good example of how simple linear (and multiple linear) regression models work under the hood: https://devarea.com/linear-regression-with-numpy/#.X6vXpq5OlhE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2136bb7f90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "\n",
    "# In this example salary is the target (y) variable (Salary) and experience is\n",
    "# the feature (X) variable.\n",
    "\n",
    "y = np.array([21,25,33,30,44,50,62,67,75,68])\n",
    "X = np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.333333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To calculate this by hand I need each metric for y = mX + b\n",
    "# Below I compute (m) the formula is:\n",
    "\n",
    "# (n * (sum(x*y)) - (sum X * sum y)) / (n * (sum X^2) - (sum X)^2)\n",
    "\n",
    "m = ((len(X) * np.sum(X*y) - np.sum(X) * np.sum(y)) /\n",
    "    (len(X)*np.sum(X*X) - np.sum(X) ** 2))\n",
    "    \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.666666666666668"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the formula is now y = 6.33 * X + b\n",
    "# next the intercept (b) is calculated using formula:\n",
    "\n",
    "# (sum y - (m * sum X)) / len(x)\n",
    "\n",
    "b = (np.sum(y) - m *np.sum(X)) / len(X)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the best fit line formula is: 6.33(X) + 12.66 \n",
    "\n",
    "def predict(x):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2136cf8b90>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graphing the best fit line\n",
    "\n",
    "vec = np.arange(10)\n",
    "plt.scatter(X,y)\n",
    "plt.plot(vec,predict(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below a function is created that mimics sklearn's LinearRegression() model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.666666666666668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getlinear(x,y):\n",
    " \n",
    "    def inner(x1):\n",
    "        return m * x1 + b\n",
    "    \n",
    "    m = (len(x) * np.sum(x*y) - np.sum(x) * np.sum(y)) / (len(x)*np.sum(x*x) - np.sum(x) * np.sum(x))\n",
    "    b = (np.sum(y) - m *np.sum(x)) / len(x)\n",
    "    return inner\n",
    "\n",
    "predict = getlinear(X,y)\n",
    "\n",
    "predict(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2136d21dd0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)\n",
    "plt.plot(X,predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple linear regression, each feature has its mean calculated unto itself in the exact same manner, Below is an example of how this would be done using two features instead of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 1 slope\n",
    "m1 = (np.sum(X2*X2)*np.sum(X1*Y) - np.sum(X1*X2)*np.sum(X2*Y))/ \\\n",
    "        (np.sum(X1*X1)*np.sum(X2*X2)-np.sum(X1*X2)*np.sum(X1*X2))\n",
    "\n",
    "# feature 2 slope\n",
    "m2 = (np.sum(X1*X1)*np.sum(X2*Y) - np.sum(X1*X2)*np.sum(X1*Y))/ \\\n",
    "        (np.sum(X1*X1)*np.sum(X2*X2)-np.sum(X1*X2)*np.sum(X1*X2))\n",
    "\n",
    "# two feature intercept calculation\n",
    "b = np.mean(Y) - a1*np.mean(X1) - a2*np.mean(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple linear regression between two variables can be quickly calculated using a native scipy function linregress()\n",
    "\n",
    "https://stackoverflow.com/questions/9538525/calculating-slopes-in-numpy-or-scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=6.333333333333333, intercept=12.666666666666671, rvalue=0.9709007393853264, pvalue=3.028721086834458e-06, stderr=0.5523137740908746)\n",
      "\n",
      "Best Fit Line: 6.33 * X + 12.67\n"
     ]
    }
   ],
   "source": [
    "# Note that the scipy.stats library has a built in linregress function\n",
    "# that will calculate all of the above at once\n",
    "\n",
    "print(scipy.stats.linregress(X, y))\n",
    "\n",
    "m = np.round(scipy.stats.linregress(X, y).slope,2)\n",
    "b = np.round(scipy.stats.linregress(X, y).intercept,2)\n",
    "\n",
    "print()\n",
    "print(f'Best Fit Line: {m} * X + {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='two'></a>2. Ridge, Lasso, and Elastic-Net Regression\n",
    "**Ridge, Lassso, and Elastic-Net** regression models are all types of 'shrinkage' estimators. Each one of these models uses regularization to counter linear regression model problems such as multicollinearity and overfitting. \n",
    "\n",
    "Simple Linear Regression and Multiple Linear Regression models use the ordinary least squares (OLS) method. This finds the coefficients that best fit line for a given data set, however OLS does not consider the relative importance of the independent variables overall. Therefore OLS models can have unnecessary variables that add noise to the results. \n",
    "\n",
    "Below is a link that describes the need for Ridge, Lasso, and Elastic Net and the pitfalls of ordinary Linear/Multiple Regression:\n",
    "\n",
    "https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db\n",
    "\n",
    "Prior to delving into each model individually, some important definitions must be examined:\n",
    "\n",
    "**Parsimonious Models** explain data with a minimum number of parameters, or predictor variables. Parsimonious models have optimal parsimony, or just the right amount of predictors needed to explain the model well (low bias, low variance). There is generally a bias/variance tradeoff between goodness of fit and parsimony: low parsimony models (i.e. models with many parameters) tend to have a better fit than high parsimony models. This is not usually a good thing; adding more parameters usually results in a good model fit for the data at hand, but that same model will likely be useless for predicting other data sets as the results can be widely unpredictable. \n",
    "\n",
    "for more on bias/variance trade-off, see the Machine Learning Overview notebook\n",
    "\n",
    "**Shrinkage** is where extreme values in a sample are “shrunk” towards a central value, like the sample mean. Shrinking data can result in:\n",
    "\n",
    "* **Shrinkage Positives**\n",
    "* Better, more stable, estimates for true population parameters\n",
    "* Reduced sampling and non-sampling errors\n",
    "* Smoothed spatial fluctuations\n",
    "\n",
    "\n",
    "* **Shrinkage Negatives**\n",
    "* Serious errors can occur if the population has an atypical mean\n",
    "* Shrunk estimators can become biased estimators, tending to underestimate the true population parameters\n",
    "* Shrunk fitted models can perform more poorly on new data sets compared to the original data set used for fitting. Specifically, r-squared “shrinks\"\n",
    "\n",
    "**Shrinkage Estimator** produces a new estimate by shrinking a raw estimate (like the sample mean). For example, two extreme mean values can be combined to make one more centralized mean value; repeating this for all means in a sample will result in a revised sample mean that has “shrunk” towards the true population mean\n",
    "\n",
    "**Regularization** is a way to avoid overfitting by penalizing high-valued regression coefficients. In simple terms, it reduces parameters and shrinks (simplifies) the model. This more streamlined, more parsimonious model will likely perform better at predictions. Regularization is necessary because least squares regression methods (where the residual sum of squares is minimized) can be unstable. This is especially true if there is multicollinearity in the model.\n",
    "\n",
    "**Alpha (sometimes Lambda)** is the regularization penalty used with Regularization models. Higher alpha's reduce the magnitude of coefficients(i.e. reduces model complexity through shrinkage)\n",
    "\n",
    "**Unbiased Models** - make weak or no assumptions about the form of the unknown underlying function that maps inputs to outputs in a dataset. Unbiased models treat all variables equally and become more complex as new variables are added, these models tend to have lower bias (fits the line well) and higher variance (unreliable predictions), thus they are prone to overfitting and subject to adding 'noise' to the analysis\n",
    "\n",
    "**Biased Models** - make strong assumptions about the form of the unknown underlying function that maps inputs to outputs in a dataset. These models will tend to have higher bias (doesn't fit the line as well) and lower variance (has consistent predictions), OLS linear regression is an example of one such model as these models are used to predict the best fit line formula (i.e. the unknown underlying function). Biased models are prone to underfitting. \n",
    "\n",
    "The image below depicts the 'sweet spot' where model bias and varaince meet. The ideal output would minimize the total error so that it comes as close as possible to this 'sweet spot'. Note that the OLS is a more complex model, therefore it will always produce results towards the right of this diagram (therefore far from the sweet spot). Regularization models (i.e. Ridge, Lasso, Elastic-Net) can be used to bring the total error closer to the sweet spot. \n",
    "\n",
    "<img src='images/ridge1.png' width=450px>\n",
    "\n",
    "---\n",
    "\n",
    "## Formula Overview\n",
    "Essentially the broad overall formula for Lasso and Ridge models is the same excepting the error term, namely:\n",
    "### $ y = XB + e$\n",
    "* y = dependent variable\n",
    "* X = independent variables\n",
    "* B = predicted regression coefficient\n",
    "* e = errors/residuals (this the penalty term and is the only difference between lasso/ridge/elastic-net models and regular OLS linear regression models. The error formula differs between each model)\n",
    "\n",
    "Note 1: y = XB is simply the normal OLS regression formula\n",
    "Note 2: if an intercept in included, it is usually left unchanged\n",
    "\n",
    "---\n",
    "\n",
    "## Ridge Regression\n",
    "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients rather than setting them to 0 or removing them altogether. \n",
    "\n",
    "Removing predictors from the model can be seen as settings their coefficients to zero. Instead of forcing them to be exactly zero, ridge regression penalizes them if they are too far from zero, thus enforcing them to be small in a continuous way. Thus, decrease model complexity while keeping all variables in the model. \n",
    "\n",
    "\n",
    "### Ridge Regression Formula\n",
    "Below is the (e) in y = XB + e for ridge regresion:\n",
    "<img src='images/ridge_math.png' width=200px>\n",
    "\n",
    "\n",
    "### Ridge Regression Models Use L2 Regularization\n",
    "Ridge Regression uses L2 regularization which adds a penalty (the alpha parameter) equal to the square of the magnitude of coefficients. L2 will not yield sparse models and all coefficients are shrunk by the same factor (none are eliminated).\n",
    "* An alpha of 0 means no penalty given and the model is essentially linear regression\n",
    "* A high alpha with ridge will penalize the parameters (through shrinkage) but not ever fully remove (or set a coeficient to 0) a particular metric. \n",
    "\n",
    "---\n",
    "\n",
    "## LASSO Regression (Least Absolute Shrinkage Selector Operator)\n",
    "**Lasso regression** is a type of **linear regression** that uses shrinkage. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination in models with a large number of features where large means either:\n",
    "1. Large enough to enhance the tendency of the model to over-fit, **a minimum of ten variables can cause overfitting**\n",
    "2. Large enough to cause computational challenges. This situation can arise in case of millions or billions of features \n",
    "\n",
    "Similar to Ridge Regression, Lasso also penalizes the absolute size of the regression coefficients. In addition, it is capable of reducing the variability and improving the accuracy of linear regression models. Lasso regression differs from ridge regression in a way that it uses absolute values in the penalty function, instead of squares. \n",
    "\n",
    "\n",
    "### Lasso Model Forumla\n",
    "<img src='images/lasso_math.png'>\n",
    "Where the formula left of the + (y = XB) is just OLS linear regression and the error (e) is the formula to the right of the +. The goal of lasso is to minimize the output of the above equation (i.e. minimizing the sum of squares using the alpha/lambda constraint)\n",
    "\n",
    "\n",
    "### L1 Regularization\n",
    "Lasso models uses L1 regularization, which adds a penalty (the alpha/lambda parameter) equal to the absolute value of the magnitude of coefficients (e in the formula). In other words, it limits the size of the coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and be eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models.\n",
    "* When alpha/lambda = 0, no parameters are eliminated, estimate is just OLS linear regression\n",
    "* As alpha/lambda increases, more and more coefficients are set to zero and eliminated (theoretically, when λ = ∞, all coefficients are eliminated) \n",
    "* As alpha increases, bias increases\n",
    "* As alpha decreases, variance increases\n",
    "\n",
    "see the link below for a good example of the alpha effect with Lasso:\n",
    "\n",
    "https://chrisalbon.com/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/\n",
    "\n",
    "---\n",
    "\n",
    "## Elastic-Net Regression\n",
    "Elastic-Net is a linear regression model trained with both L1 and L2\n",
    "regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge, effectively getting the best of both worlds.\n",
    "\n",
    "Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both.\n",
    "\n",
    "A practical advantage of trading-off between Lasso and Ridge is that it allows Elastic-Net to inherit some of Ridge’s stability under rotation.\n",
    "\n",
    "---\n",
    "\n",
    "## Important Usage Points\n",
    "* All three models can be useful when there are high levels of muticollinearity between feature variables, especially if this is causing unreliability in a linear regression model\n",
    "* Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (i.e. when only a few predictors actually influence the response)\n",
    "* Ridge works well if there are many large parameters of about the same value (i.e. when most predictors impact the response)\n",
    "* The size of respective alpha values can be tuned via cross-validation to find the ideal alpha for a given data set\n",
    "\n",
    "\n",
    "## Lasso, Ridge, and Elastic-Net Model Evaluation Metrics\n",
    "Each one of these models attempt to rectify issues with Linear Regression models. Therefore, the same evaluation metrics used with multiple linear regression models will work for these as well\n",
    "\n",
    "\n",
    "## Linear Regression Example with High Multicolinearity\n",
    "Because Lasso, Ridge, and Elastic-Net are all used to fix issues with linear regression models that contain highly correlated feature variables I first want to show an example of data with high collinearity.\n",
    "\n",
    "Note that depending on the situation, it may not be an issue if there is slight or moderatae collinearity. However, if correlation between two features is >0.8 or the Variance Inflation Factor (VIF) >20 then it is advised to solve the issue. \n",
    "\n",
    "https://towardsdatascience.com/multi-collinearity-in-regression-fe7a2c1467ea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='three'></a>3. Polynomial Regression\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable X and the dependent variable y is modelled as an nth degree polynomial in X. Polynomial regression fits a **non-linear** relationship between the value of X and the corresponding conditional mean of y. A polynomial regression equation if the power of independent X variable is >1. Transalation: use polynomial regression if there is some form of relationship (or pattern) other than linear between two data points.\n",
    "\n",
    "---\n",
    "\n",
    "##  Polynomial Regression Formula: y = b + mX^2\n",
    "\n",
    "* Where, y = dependent variable, X = independent variable, b = y-intercept, m = slope of the line (or coefficient)\n",
    "\n",
    "\n",
    "With polynomial regression, the 'best fit line' is not straight but rather a curve that fits to exponential data points. Note that the number of line curves correspont with the value of the X's nth degree (or value), see below graph for examples:\n",
    "\n",
    "<img src='images/poly1.png'>\n",
    "\n",
    "---\n",
    "\n",
    "## Common use-cases for Polynomial regression are:\n",
    "\n",
    "* **Business trend analysis and forecasting** (example, company trying to predict sales numbers based on amount spent on advertising or estimating growth in sales future sales based on current economic conditions)\n",
    "* **Time Series Modeling** (stock prices)\n",
    "* **Pricing change impact on business**\n",
    "* **Risk assesments** (insurance, financial services, etc.)\n",
    "* **Numerical weather predictions** (what tempurature tomorrow)\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions of Polynomial Regression\n",
    "All of the assumptions for linear regression hold for polynomial regression except:\n",
    "* Polynomial regression does not require the X and y variables to be linearly related\n",
    "* The level of degree will increase the fit of the model, however it can also cause under/over fitting. \n",
    "\n",
    "### How to Determine Optimal nth degree for X\n",
    "There are two techniques used in deciding the optimal degree for a polynomial equation:\n",
    "* Forward Selection - a method where the degree is increased until it is significant enough to define the model\n",
    "* Backward Selection - a method where the degree is decreased until it is significant enough to define the model\n",
    "\n",
    "## Model evaluation is handled in the same manner as linear regression, i.e RMSE, MAE, R2, ect. \n",
    "\n",
    "## IMPORTANT NOTE\n",
    "Polynomial Regression is used for POLYNOMIAL FEATURES not target variables. Most of the below example is data wrangling, but in the end I incorrectly tried to use Polynomial regression by using my polynomial feature (prices) as my target variables. Remember, with Regression, only NORMALLY DISTRIBUTED data can be used as target variable/s\n",
    "\n",
    "---\n",
    "# TODO: Example of polynomial regression between two variables\n",
    "\n",
    "\n",
    "Good Link Explaining Polynomial Regression\n",
    "\n",
    "https://www.kaggle.com/thaddeussegura/enough-to-be-dangerous-polynomial-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='four'></a>4. Generalized Linear Models (GLM)\n",
    "The term generalized linear model usually refers to conventional linear regression, however, this term can also be attributed to a larger class of models (often nonlinear).\n",
    "\n",
    "There are three components to any GLM:\n",
    "1. **Random Component** - refers to the probability distribution of the response variable (Y); e.g. normal distribution for Y in the linear regression, or binomial distribution for Y in the binary logistic regression\n",
    "2. **Systematic Component** - specifies the explanatory variables (X1, X2, ... Xk) in the model, more specifically their linear combination in creating the so called linear predictor\n",
    "3. **Link Function** - specifies the link between random and systematic components. It says how the expected value of the response relates to the linear predictor of explanatory variables\n",
    "\n",
    "\n",
    "**The table below shows most GLM's, note that not all GLM's are linear:**\n",
    "<img src='images/gen1.png' width='600'>\n",
    "\n",
    "---\n",
    "### ANOVA (Analysis of Variance)\n",
    "Analysis of Variance is a process of examining the difference among the means of multiple groups of data for homogeneity. ANOVA performs just like linear regression however it is used to predict a continuous outcome variable from one or more categorical features. (NOTE THE FEATURES MUST ALL BE CATEGORICAL)\n",
    "* **USE THIS IF ALL FEATURES ARE CATEGORICAL BUT TARGET IS CONTINUOUS**\n",
    "\n",
    "---\n",
    "### ANCOVA (Analysis of Covariance)\n",
    "ANCOVA is a blend of analysis of variance (ANOVA) and regression and it can be used as:\n",
    "* An extension of multiple regression to compare multiple regression lines. When used as such, ANCOVA can test all of the regression lines to see which have different intercepts as long as the slopes for each line is the same. Like regression analysis, ANCOVA enbales one to see how features act on a target variable and can remove the effect of any covariates (that is, variable you don't really need)\n",
    "\n",
    "\n",
    "* As an extension of ANOVA, ANCOVA can be used to control for covariates that aren't the main focus of your study and/or to study combinations of categorical and continuous variables on a scale as predictors (in this case the covariate is the variable of interest), just like ANOVA, ANCOVA features must be categorical (minimum of two) and the target should be continuous\n",
    "\n",
    "---\n",
    "### Log-Linear Regression\n",
    "is a model that takes the form of a function whose log is a linear combination (or expression constructed from a set of terms by multiplying each term by a constant and adding the results) of the parameters of the model\n",
    "\n",
    "---\n",
    "### Poisson Regression\n",
    "A **Poisson Distribution** is a discrete (non-continous) probability distribution that expresses the probability of a given number of events occuring in a fixed interval of time or space, with a known constant mean rate and independent of the time since the last event (i.e., roll of a die, non-continous set possible values and every role is independent from the next)\n",
    "\n",
    "**Poisson Regression** is a special type of regression in which the target variable consists of 'count data'. Below are some examples when such a regression could be used:\n",
    "* Determine the number of students (target count variable) who graduate based upon gender (male/female)\n",
    "\n",
    "\n",
    "* Number of traffic accidents (target count var) based on weather (sunny/rain) and if a special event is taking place (yes/no)\n",
    "\n",
    "For **continuous** predictor variables, the results will tell how a one unit increase or decrease in that variable is associated with a percentage change in the counts of the target variable (i.e. each additional point increase in GPA is associated with a 12.5% increase in number of students who graduate)\n",
    "\n",
    "For **categorical** predictor variables, the results will interpret the percentage change in counts of one group compared to another group (i.e. # of people who finish a race in sun VS. # of peple who finish a race in rain)\n",
    "\n",
    "**Assumptions of Poisson Regression**\n",
    "* Target variable consists of count data (note in traditional linear regression target variables conssit of continuous data)\n",
    "\n",
    "\n",
    "* Features are independent of one another\n",
    "\n",
    "\n",
    "* The distribution of counts must follow a Poisson Distribution (that is, the observed expected counts should be similar)\n",
    "\n",
    "\n",
    "* The mean and variance of the model are equal\n",
    "\n",
    "---\n",
    "### Multinomial Response Regression\n",
    "multinomial logistic regression is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables (which may be real-valued, binary-valued, categorical-valued, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='five'></a>5. Bayesian Regression\n",
    "In order to understand Bayesian Regression, Bayes' theorem and Bayesian inference must first be examined:\n",
    "\n",
    "**Bayes' Theorem** describes the probability of an event based on prior knowledge of conditions that might be related to the event (for example, if the risk of developing health problems as age increases is known, then risk to an individual based on their age can be predicted with some accuracy\n",
    "\n",
    "**Bayesian Inference** is a method of statistical inference in which Bayes' theorem is used to update the probability for an ongoing hypothesis test as more evidence or information becomes available\n",
    "\n",
    "**Bayesian Linear Regression** is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. Bayesian Regression uses probability distributions rather than point estimates. The response variable (y) is not estimated as a single value, but is assumed to be drawn from a probability distribution.\n",
    "\n",
    "The aim of Bayesian Linear Regression is not to find the single “best” value of the model parameters, but rather to determine the posterior distribution for the model parameters. \n",
    "\n",
    "**Posterior Distribution** is the probability distribution of an unknown quantity, treated as a random variable, conditional on the evidence obtained from an experiment or survey.\n",
    "\n",
    "For clarity of the above, in Bayesian statistics, the **posterior probability** of a random event or an uncertain proposition is the conditional probability that is assigned after the relevant evidence or background is taken into account. \"Posterior\", in this context, means after taking into account the relevant evidence related to the particular case being examined. \n",
    "\n",
    "The general idea of the Bayesian regression process is to start with an initial estimate (from prior existing data), and as more evidence or data is gathered the model becomes less wrong. Bayesian reasoning is a natural extension of our intuition.\n",
    "\n",
    "The key takeaway here is Bayesian Regression is best used when:\n",
    "* **Prior data is available along with reasonable probability assumptions for predicting future outcomes**\n",
    "\n",
    "Link to example:\n",
    "\n",
    "https://www.kaggle.com/sathi557/bayesian-linear-regression-demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name='six'></a>6. Least Angle Regression\n",
    "LARS is a regression algorithm used for high-dimensional data and is similar to forward stepwise regression. At each step, it finds the feature most correlated with the target. When there are multiple features having equal correlation, instead of continuing along the same feature, it proceeds in a direction equiangular between the features.\n",
    "\n",
    "**Some advantages of LARS are:**\n",
    "* It is efficient when the number of features is significantly greater than the number of samples\n",
    "* It is computationally just as fast as forward selection and has the same order of complexity as ordinary least squares\n",
    "* It produces a full piecewise linear solution path, which is useful in cross-validation or similar attempts to tune the model\n",
    "* If two features are almost equally correlated with the target, then their coefficients should increase at approximately the same rate. The algorithm thus behaves as intuition would expect, and also is more stable.\n",
    "* It is easily modified to produce solutions for other estimators, like the Lasso.\n",
    "\n",
    "**Some disadvantages of LARS are:**\n",
    "* Susceptible to noise (outliers, ect.) as it is based upon an iterative refitting of the residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='seven'></a>7. Support Vector Machine Regression (SVR)\n",
    "Support Vector Machines Regression (SVR) gives the flexibilty to define how much error is acceptable in the model results and will find an appropriate regression line to fit the data (for example, if an error of $5,000 is acceptable when predicting housing prices, then SVR is useful). \n",
    "\n",
    "This powerful algorithm that allows us to choose how tolerant we are of errors, both through an acceptable error margin (ϵ) and through tuning our tolerance of falling outside that acceptable error rate (Constraints).\n",
    "\n",
    "**Margin of Error ($\\epsilon$)** - is a hyperparameter for SVR models that represents the acceptable error range \n",
    "\n",
    "**Constraints** - is a hyperparameter used to constrain any values that fall outside of $\\epsilon$ (i.e. this allows the user to set the overall acceptable data range and can help with outliers is some situations)\n",
    "\n",
    "Below is a graph where many different Constraint values are tested, the ultimate goal here is to maximize the percent of data that falls within the acceptable $\\epsilon$ range (note that MAE is being tested here):\n",
    "\n",
    "<img src='images/svr1.png'>\n",
    "\n",
    "From the image above is it clear that MAE increases as C increases but the maximum data percent within $\\epsilon$ maxes out when C is around 6.13, therefore the ideal hyperparameters for this model are $\\epsilon$ = 5 and C = 6.13. Below is a graph os what this would look like:\n",
    "\n",
    "<img src='images/svr2.png'>\n",
    "\n",
    "**Some advanatages of SVM's are:**\n",
    "\n",
    "* They are effective with high dimensional data\n",
    "* They are effective when # of dimensions is greater than # of samples\n",
    "* They are memory efficient due to using subset training points (aka: support vectors)\n",
    "* They are versatile and can use different Kernel functions (linear/polynomial/sigmoid/ect.) including custom kernals\n",
    "\n",
    "**Some disadvantages to SVM's are:**\n",
    "* If # of features is much greater than # of samples, choosing most appropriate kernal functions adn regularization is crucial to avoid overfitting\n",
    "* SVM's do not directly provide probabiliy estimates, these are calculated using (a memory expensive) five-fold cross-validation process\n",
    "\n",
    "This link does a good job explaining SVR's, epsilon, and constraints both theoretically and visually:\n",
    "https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2\n",
    "\n",
    "\n",
    "## Looks like a good example for classification\n",
    "https://www.kaggle.com/bandiatindra/telecom-churn-prediction\n",
    "\n",
    "\n",
    "## Looks like a good example for regression\n",
    "https://www.kaggle.com/abhijithchandradas/xgboost-vs-linear-regression-vs-svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='eight'></a>8. Stochastic Gradient Descent Regression (SGD)\n",
    "Stochastic gradient descent is a simple yet very efficient approach to fit linear models. It is particularly useful when the number of samples (and the number of features) is very large.\n",
    "\n",
    "Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. \n",
    "\n",
    "**Optimizers** are algorithms that adjust the weights of features to minimize the overall loss function (i.e. minimize MAE, RMSE, ect.)\n",
    "\n",
    "This optimization is used when features have specific weights to them and if/when a bias is present. The job of the optimizer is to adjust the weights (and bias if necessary) to minimize the loss.\n",
    "\n",
    "Often, an instance of SGDClassifier or SGDRegressor will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. \n",
    "\n",
    "For example, using SGDClassifier(loss='log') results in logistic regression, i.e. a model equivalent to LogisticRegression which is fitted via SGD instead of being fitted by one of the other solvers in LogisticRegression.\n",
    "\n",
    "Similarly, SGDRegressor(loss='squared_loss', penalty='l2') and Ridge solve the same optimization problem, via different means.\n",
    "\n",
    "**IN OTHER WORDS, THERE ARE ALREADY BUILT IN MODELS THAT PERFORM THE SAME AS SOME SGD OPTIMIZATION SETTINGS. THEREFORE, SGD SHOULD ONLY BE USED IF THOSE MODELS ARE NOT SUFFICIENT AND MORE (OR DIFFERENT) OPTIMIZATION IS REQUIRED**\n",
    "\n",
    "Here is a good example where SGD Regresssion optimization is used:\n",
    "https://www.kaggle.com/ryanholbrook/stochastic-gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='nine'></a>9. Nearest Neighbor Regression\n",
    "\n",
    "In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric alogrithm (no assumptions about data made) used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether KNN is used for classification or regression:\n",
    "\n",
    "* In KNN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "* In KNN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n",
    "\n",
    "**Some advanatages of KNN are:**\n",
    "* No assumptions are made about the data\n",
    "* KNN can be used for both regression and classification\n",
    "* KNN is simple to use and is often highly accurate\n",
    "* There is no splitting of the data into training and test sets (i.e. all training data is used)\n",
    "\n",
    "**Some disadvantages to KNN are:**\n",
    "* Computationally expensive for large datasets\n",
    "* Sensetive to the scale of the dataset and can be thrown off by irrelevant features quite easily \n",
    "* Choosing the incorrect value of K (the wrong number of neighbors to consider, or the number of potentially different class choices overall) can result in bad predictions, thus choosing the proper value of k is extremely important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='ten'></a>10. Gaussian Process Regression (GPR)\n",
    "GPR are generic supervised learning methods designed to solve regression and probabilistic classification problems.\n",
    "\n",
    "**Gaussian Process** is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed.\n",
    "\n",
    "A machine-learning algorithm that involves a Gaussian process uses lazy learning and a measure of the similarity between points (the kernel function) to predict the value for an unseen point from training data. The prediction is not just an estimate for that point, but also has uncertainty information—it is a one-dimensional Gaussian distribution. For multi-output predictions, multivariate Gaussian processes are used, for which the multivariate Gaussian distribution is the marginal distribution at each point. \n",
    "\n",
    "\n",
    "\n",
    "**Some advantages of GPR's are:**\n",
    "* The prediction interpolates the observations (at least for regular kernels)\n",
    "* The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest\n",
    "* Versatile: different kernels can be specified. Common kernels are provided, but it is also possible to specify custom kernels\n",
    "\n",
    "**Some disadvantages of GPR's are:**\n",
    "* Computationally Expensive\n",
    "* They are not sparse (i.e. non-parametric), they use the whole samples/features information to perform the prediction\n",
    "* They lose efficiency in high dimensional spaces – namely when the number of features exceeds a few dozens\n",
    "\n",
    "\n",
    "Below is a good link that explains Bayesian Inference and Gaussian Process well:\n",
    "https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='eleven'></a>11. Decision Tree Regression\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "**Some advantages of Decision Trees are:**\n",
    "* Simple to understand and interpret, can also be visualized\n",
    "* Requires little data preparation (no normalization, dummy vars, ect.)\n",
    "* Can handle both numerical and categorical data\n",
    "* can handle multi-output problems\n",
    "\n",
    "**Some disadvanatages of Decistion Trees are:**\n",
    "* Decision-tree learners can create over-complex trees that do not generalise the data well (overfitting). Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the **maximum depth** of the tree are necessary to avoid this problem\n",
    "* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble\n",
    "* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree\n",
    "\n",
    "\n",
    "\n",
    "Link to Explore more:\n",
    "\n",
    "https://www.kaggle.com/thaddeussegura/enough-to-be-dangerous-decision-tree-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#  <a name='twelve'></a>12. Random Forest Regression\n",
    "**Random Forest** is an ensemble algorithm capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging.\n",
    "\n",
    "The basic idea behind Random Forest Tree analysis is to combine multiple decision trees in determining the final output rather than relying on individual decision trees. when splitting each node during the construction of a tree, the best split is found either from all input features or from a random subset size.\n",
    "\n",
    "Note that individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n",
    "\n",
    "**Bagging** involves training each decision tree on a different data sample where sampling is done with replacement (note this is bagging in relation to Random Forest only)\n",
    "\n",
    "**Random Forest Regression** is an ensemble of decision trees where:\n",
    "* Each tree is created from a different sample of rows and at each node, a different sample of features is selected for splitting (the randomness)\n",
    "* Each tree makes its own individual prediction\n",
    "* The predictions are averaged together for a single result\n",
    "\n",
    "\n",
    "### Use Random Forest Regression when the data has a non-linear trend and extrapolation (unknown inferences) outside of the tranining data is not important\n",
    "\n",
    "### Avoid Random Forest Regression when using time-series data\n",
    "\n",
    "Link To Explore More:\n",
    "\n",
    "https://www.kaggle.com/thaddeussegura/enough-to-be-dangerous-random-forest-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
