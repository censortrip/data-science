{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Overview\n",
    "## Part I - Statistics Basics & Descriptive Statistics\n",
    "---\n",
    "Statistics is an area of applied mathematics concerned with data collection, analysis, inerpretation, and presentation \n",
    "\n",
    "It consists of Three Different Notebooks: \n",
    "\n",
    "* **Part I   - Statistics Basics**\n",
    "* **Part II  - Descriptive Statistics**\n",
    "* **Part III  - Probability**\n",
    "* **Part IV - Inferential Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part I - Statistics Basics\n",
    "---\n",
    "\n",
    "**Data Set** - The data collected for a particular study\n",
    "\n",
    "**Elements** - Individual values within a data set\n",
    "\n",
    "**Variable** - the column containing a group of elements, more specifically the name of the column which characterizes the data for that column.\n",
    "\n",
    "**Population** - set of all elements about which we wish to draw conclusions\n",
    "\n",
    "**Sample** - subset of the elements of a population\n",
    "\n",
    "**Quantitative Data** -  data that can be counted, measured, and expressed using numbers (for example: 'how much of something', or 'how many')\n",
    "\n",
    "**Quantiative Variables** - there are two types of quantiative variables.\n",
    "* **Discrete (aka Categorical)** - Data that cannot be broken down into smaller parts, i.e. holds a finite number of possible values. (Example: number of iphones sold last year, number of students in a class)\n",
    "\n",
    "* **Continuous** - Data that can be infinitely broken down into smaller parts. (Example: weight, age). \n",
    "\n",
    "**Qualitative Data** - data that can be categorized based on traits and characteristics (examples: person's gender, car model, etc.)\n",
    "\n",
    "**Qualitative (Categorical) Variables** - there are two types of categorical variables.\n",
    "* **Ordinal** - a qualitative variable with meaningful ordering and ranking. (Example: Satisfaction Rating like: 'happy', 'unhappy'). Numbers can also be used like 1 for happy and 0 for unhappy, note that orginal number can not be quantified. \n",
    "\n",
    "* **Nominal** - a qualitative variable with no meaningful ordering or ranking. (Example: gender, car model colors).\n",
    "\n",
    "**Cross Sectional Data** - is data collected at the same or approximately around the same time. (example: multiple company stock closing prices for one day)\n",
    "\n",
    "**Time Series Data** - is data collected over different periods of time. (Example: daily stock returns for one company for a month)\n",
    "\n",
    "---\n",
    "## Data Sources\n",
    "Oftentimes data is available publicly or for sale privately. When this is the case the data is an **existing** data source. \n",
    "On the other hand, if data is not readily available then one must collect the data themselves. Often this is done through **surveys** or **experimental studies**.\n",
    "\n",
    "**Existing Data** - data that has already been gathered from an outside source\n",
    "\n",
    "**Response Variable** - main varible (column) of interest in a study\n",
    "\n",
    "**Factors** - other variables that could potentially relate to the response variable\n",
    "\n",
    "**Experimental Study (Supervised)** - when factors of a response variable are able to be set and manipulated, then a study is said to be experimental (Example: Drug compnay testing medications and using placebos)\n",
    "\n",
    "**Observational study (Unsupervised)** - when factors of interest are unable to be controlled, a study is said to be observational. (Example: cholesterp and diet studies)\n",
    "\n",
    "---\n",
    "## Sampling Techniques\n",
    "Sampling is a statistical method that deals with a selection of observations from within a population. There are two main types of sampling methodologies, Probability and Non-Probability. Probability sampling is the method most often used in conjunction with statistics. There are 3 main types of Probability sampling:\n",
    "\n",
    "**Random Sampling** - Each element or member of a population has an equal chance of being selected for the sample\n",
    "\n",
    "**Systematic Sampling** - Every nth record is chosen from the population to be a part of the sample\n",
    "\n",
    "**Stratified Sampling** - A stratum (or subset) of the population that shares at least one common characteristic (such as gender or age) is created, then random sampling is used to select a sufficient number of subjects from the stratum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Part II - Descriptive Statistics\n",
    "---\n",
    "The science of describing the important aspects of a set of data  **Statstical Inference**, uses descriptive statistics in order to make genralizations about the important aspects of a population (or sample) of data. \n",
    "\n",
    "**Descriptive Statistics** are measured both graphically and numerically and often both methods are intertwined: \n",
    "\n",
    "## Graphical Methods of Descriptive Statistics \n",
    "* **Quantitative Data** - quantitative data can be graphed using Histograms, Line Charts, and Scatter Plots to name a few. \n",
    "\n",
    "\n",
    "* **Qualtitaive Data** - qualitative data can be graphed using Bar Charts, Pie Charts, Point Plots, Categorical Scatter Plots, Box Plots, and Violin Plots to name a few. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "##  Quantitative Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE SOME SAMPLE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "##  Qualitative Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pie Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catgorical Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Numerical Methods of Descriptive Statistics \n",
    "Numerical analysis with descriptive statstics is used to describe the **central tendency** of a data set (or the typical value within a data set). \n",
    "\n",
    "**Parameter** - is a number calculated that descrives some aspect of a population or sample, example mean, median, mode\n",
    "\n",
    "**Mean** - simple average of an aspect from a population or sample. \n",
    "\n",
    "**Median** - the center value, if data set lenght is odd, then median is central value, if data set is even, then median is average of the two central data points\n",
    "\n",
    "**IMPORTANT NOTE:**\n",
    "The median is generally resistant to extreme (outlier) values whereas the mean is not. Therefore is data sets with wide ranges, the median is often the best choice for central tendecy measurements. \n",
    "\n",
    "**Mode** - the value that appears the most. \n",
    "\n",
    "**Min** - the smallest value in a data set. \n",
    "\n",
    "**Max** - the largest value in a data set. \n",
    "\n",
    "**Range** - The largest value minus the smallest value in a data set.\n",
    "\n",
    "**Variance** - The average of the squared differences from the mean (variance describes how much a random variable differs from its expected value)\n",
    "\n",
    "**Standard Deviation (STD)** - Measures the extent of deviation of a group of data as a whole from the mean, and is measured by the positive squared root of the variance.\n",
    "\n",
    "**Coefficient of Variation (aka Relative Standard Deviation)** - a stadardized measure of dispertion of a frequency or probability distribution. This measure tells how varied a set of data is. It is calculated by:\n",
    "\n",
    "* **STD / mean * 100**\n",
    "\n",
    "The coefficient of Variation is useful when comparing separate samples or populations with different means and standard deviations, Below are some examples. \n",
    "\n",
    "<img src='data/images/coeff_var.PNG'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "One type of relative frequency curve (or probabily distribution) describing a data set is the normal curve, which is a symmetrical, bell-shaped curve. If a population or sample can be described by the normal curve, we say that the data is normaly distributed.\n",
    "\n",
    "Note: A **Normal Random Variable** is a variable with mean at 0 and variance equal to 1\n",
    "\n",
    "The **mean** of a normal distribution determines the location of the center of the graph and the **Standard Deviation** determines the height of the graph (large std - short wide curve, small std = tall and narrow\n",
    "\n",
    "The **Central Limit Theorem** states that the sampling distribution of the mean of any independent random variable will be normal or nearly normal, if the sample size is big enough\n",
    "\n",
    "**Empirical Rule for Normal Distribution** - states that for a normal distribution, nearly all of the data will fall within three standard deviations of the mean. It states:\n",
    "* 68.26% of a populations measurements will fall within 1 STD of the mean. \n",
    "* 95.44% of a populations measurements will fall within 2 STD of the mean.\n",
    "* 99.73% of a populations measurements will fall within 3 STD of the mean. \n",
    "\n",
    "**Note that the Emprical rule only holds for normal bell shaped distibutions.**\n",
    "\n",
    "\n",
    "<img src='data/images/emp_rule.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness & Kurtosis\n",
    "**Skewness** - measures the symmetry or asymmetry of a bell curve. There is a formula to calculate skewness and the results can be interpreted as follows:\n",
    "* skewness < -1 OR > 1, highly skewed\n",
    "* skewness between -1 and -0.5 OR 0.5 and 1, moderately skewed\n",
    "* skewness between -0.5 and 0.5, approximately symmetric\n",
    "\n",
    "**Symmetrical Curve** - perfect bell shape with no skew.\n",
    "* Mean = Median = Mode\n",
    "\n",
    "**Left Skew (aka negative skew)** - has disproportionate tail to the left.\n",
    "* Mean < Median < Mode \n",
    "\n",
    "**Right Skew (aka positive skew)** - has disproportionate tail to the right, also known as positively skewed.\n",
    "* Mean > Median > Mode\n",
    "\n",
    "**Skewness Diagram**\n",
    "<img src='data/images/skew.png'>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Kurtosis** - a measure of whether data distrition is heavy-tailed or light-tailed relative to a normal distribution.\n",
    "* kurtosis = 3, normally distributed\n",
    "* kurtosis > 3, fat-tailed distribution\n",
    "* kurtosis < 3, thin-tailed distribution\n",
    "* genrally acceptable range for kurtosis is -2 to 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## z-score (or standardized value)\n",
    "A z-score is the number of standard deviations that x is from the mean:\n",
    "* **z-score = (x - mean) / standard deviation**\n",
    "\n",
    "Z-scores also denote relative location between samples. For example, values from two different populations or samples with the same z-score are therefore the same number of std's away from their respective means. Below is a z-score breakdown:\n",
    "\n",
    "* z-score = 0; x is on the mean\n",
    "\n",
    "\n",
    "* positive z-score; x > average mean, so if z = 2, x is 2 std's above mean\n",
    "\n",
    "\n",
    "* negative z-score; x < average mean, so if z  = -1, x is 1 std below mean \n",
    "\n",
    "To determine the exact % value that x is from the mean, use the z-score table, but just knowing 1, 2, and 3 std's will give you a good idea of where the sample lies.\n",
    "\n",
    "A good reference link explaining z-scores and how to use them in depth:\n",
    "https://www.statisticshowto.com/probability-and-statistics/z-score/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two methods for caclulating Outliers using z-score\n",
    "\n",
    "# Method 1, Use Scipy Stats\n",
    "#z = np.abs(stats.zscore(df))\n",
    "\n",
    "# Set threshold to return values > 3 std's from the mean\n",
    "#np.where(z > 3)\n",
    "\n",
    "# To Get max or min vals, use numpy\n",
    "#np.min(z)\n",
    "#np.max(z)\n",
    "\n",
    "#----------------------------------------------------\n",
    "# Method 2, Manual Calucation Python list\n",
    "\n",
    "#outliers=[]\n",
    "#scores = list(scrdf['imdb_score'])\n",
    "\n",
    "#for score in scores:\n",
    "#    z_score = (score - 6.5)/1.0\n",
    "#    if np.abs(z_score) > 3.0:\n",
    "#        outliers.append(score)\n",
    "#    else:\n",
    "#        pass\n",
    "\n",
    "#print(max(outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentiles, Quartiles, and the Inter Quartile Range\n",
    "a percentile is a value x such that a a percentage of a population or sample falls below. \n",
    "\n",
    "In many cases, it can be helpful to divide a data set up into 4 sections by percentile into four sections are called Quartiles that altogether make up the Inter Quartile Range:\n",
    "\n",
    "* 1st Quartile, the 25th percentile or the value where 25% or less of the data lies\n",
    "* 2nd Quartile, the 50th percentile (THE MEDIAN), or the value where 50% or less of the data lies. \n",
    "* 3rd Quartile, the 75th percentile, or the value where 75% or less of the data lies. \n",
    "* 4th Quartile, the 100th percentile (Max Value), all other data points, lie below this value. \n",
    "\n",
    "**IMPORTANT NOTE**\n",
    "In general percentiles are less suseptable to noise and outliers than other values. In cases where a population or sample is highly skewed (either left or right), then the standard deviation is usually so large that it cannot be used to provide any useful information. In such cases, population variation is best described using percentiles over standard deviation\n",
    "\n",
    "The graphical display of the Quartile range is the Box Plot (aka Box and Whiskers), this plot also is good for detecting outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "In many cases we want two analyze two separate variables (columns) from a popultion or sample in order to see if they move togther or not. If two samples do move together (that is when one samples values increase so does the other and vice-versa) then they are said to have a **linear relationship.**\n",
    "\n",
    "**Covariance** - is a measure of the direction of linearity between variables x and y (ex: positive linearity, negative linearity), however is does not distinguish the strength of the linear relationship.  \n",
    "\n",
    "**Corellation Coefficient** - a measures both the direction and strenght of a linear realtionship between x and y. Corellation Coefficinets will always be between -1 and 1. The scores are intepreted below:\n",
    "\n",
    "* -1.0 = perfect negative linear relationship\n",
    "* -0.7 = strong negative linear relationship\n",
    "* -0.5 = moderate negative linear relationship\n",
    "* -0.3 = weak negative linear relationship\n",
    "*  0.0 = no linear relationship\n",
    "*  0.3 = weak positive linear relationship\n",
    "*  0.5 = moderate positive linear relationship\n",
    "*  0.7 = stong positive linear relationship\n",
    "*  1.0 = perfect linear relationship\n",
    "\n",
    "Note that negative linear relationships move downhill from left to right whereas positive linear relationships move uphill from left to right. See image below:\n",
    "\n",
    "<img src='data/images/corr_coeff.png' width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Weighted Mean\n",
    "Often times we compare values based on percentages. One simple example is a teacher who has a weighted grading system like so:\n",
    "\n",
    "* homework = 25\n",
    "* quiz = 10\n",
    "* tests = 30\n",
    "* final exam = 35\n",
    "\n",
    "Note that all elements above = 100% for the final grade. Because not all values are treated equally, then we can't use simple averages to compute the final grade. \n",
    "\n",
    "For example, say a student gets these grades:\n",
    "\n",
    "* homework = 88\n",
    "* quiz = 71\n",
    "* tests = 97\n",
    "* final exam = 90\n",
    "\n",
    "Taking the simple average would give us: \n",
    "\n",
    "$ (88 + 71 + 97 + 90) / 4 = 86.75 $ \n",
    "\n",
    "However, this is incorrect because of unequal weighting for each category. To take the weights into account, the averaging formula must be altered to:\n",
    "\n",
    "$ \\sum (w * x) / \\sum w $ \n",
    "\n",
    "Which is:\n",
    "\n",
    "(88 * 25) + (71 * 10) + (97 * 30) + (90 * 25) / 100 = 89.7\n",
    "\n",
    "Notice that the weighted score is 3 points higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating weighted mean\n",
    "\n",
    "weights = np.array([25, 10, 30, 35])\n",
    "s1_grades = np.array([88, 71, 97, 90])\n",
    "\n",
    "sum(weights * s1_grades) / sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped Data\n",
    "Central tendency and variability is usually measured using individual values from within a population or sample. However, often times the only data available is in the form of a frequency distribution bar chart or historgram. \n",
    "\n",
    "Data summarized in frequency distribution or histogram form are often called grouped data. \n",
    "\n",
    "Suppose you have a frequency distribution summarizing a sample of 65 customer satisfaction ratings for a product like so:\n",
    "* Ratings : Frequency\n",
    "* 36 - 38 : count 4\n",
    "* 39 - 41 : count 15\n",
    "* 42 - 44 : count 25\n",
    "* 45 - 47 : count 19\n",
    "* 48 - 50 : count 2\n",
    "\n",
    "Because we do not know the each of the 65 individual ratings, we cannot compute the exact mean satisfaction rating. But, we can calculate an approximation of this mean. \n",
    "\n",
    "To do so, the midpoint of each ratings class is used to represent the measurement for that class. the midpoints are multiplied by each frequency than summed together and dived by the total number of samples. Here's the formula:\n",
    "\n",
    "$ \\sum $ **(freq * midpoint) / total samples**\n",
    "\n",
    "\n",
    "<img src='data/images/mean.jpg' width=\"600\">\n",
    "\n",
    "\n",
    "The Variance can also be approximated in a similar manner:\n",
    "\n",
    "<img src='data/images/var.jpg' width=\"600\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pandas\n",
    "#c6_s1 = c6.sample(n = 700, replace=False)  3 replace= False means no duplicate rows\n",
    "\n",
    "# np.choice() also creates random samples from a population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3 - Probability\n",
    "\n",
    "---\n",
    "\n",
    "For an in depth dive into probability including formulas and examples, go to the notebook in the Harvard CS50 Intro to AI Part III - Uncertainty, only the very basics are touched on below\n",
    "\n",
    "**Probability** - the ratio of desired outcomes to total outcomes (desired outcomes / total outcomes). Probabilty of all outcomes always sums to 1\n",
    "\n",
    "**Probabilty In Practice** - There are 3 steps used to calculate probability:\n",
    "1. **Random Experiment** - an experiment or process for which the outcome cannot be predicted with certainty\n",
    "2. **Sample Space** - The entire possible set of outcomes of a random experiment \n",
    "3. **Event** - One or more outcomes of an experiment (a subset of the Sample Space)\n",
    "\n",
    "**Example - Rolling a Die**\n",
    "1. **Random Experiment** - Probabilty of getting a 2 when rolling a die\n",
    "2. **Sample Space** - All values on die face (1 - 6 with standard die)\n",
    "3. **Event** - The Probability of getting a two when rolling a die, or 1/6\n",
    "\n",
    "There are two types of **Events**:\n",
    "1. **Disjoint Events** - events that do not have any common outcomes (A single card cannot be a king and a queen, a man can not be dead and alive)\n",
    "2. **Non-Disjoint Events** - events that can have common outcomes (a student can get 100 on a math test and 100 on an english test)\n",
    "\n",
    "## Three Types of Probability:\n",
    "1. **Marginal Probability (aka Unconditional Probability)** is the probability of the occurrence of a single event (example, probability of drawing a heart from a deck of cards would be 13/52 \n",
    "2. **Joint Probability** is the probability of two events happening at the same time \n",
    "3. **Conditional Probability** is the probability of an event or outcome occurring based on the occurrence of a previous event or outcome\n",
    "\n",
    "**A Good example of each is given in the video at: 1:35:56**\n",
    "\n",
    "## Bayes' Theorem\n",
    "Bayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "## Probability Density Function\n",
    "**Probabilty Density Function** is the equation describing a contiuous probabilty distribution (A Bell Curve or Normal Distribution is an example of this. There are 3 properties of a PDF:\n",
    "1. A PDF graph will be continuous over a range\n",
    "2. The area bounded by the curve of a density function and the x-axis is equal to 1\n",
    "3. The probabilty that a random variable assumes a value between a & b is equal to the area under the PDF bounded by a & b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4 - Inferential Statistics\n",
    "\n",
    "---\n",
    "Descriptive statistics describe data (for example, a chart or graph) while**inferential statistics** allows you to make predictions (\"inferences\") from that data. With inferential statistics, you take data from samples and make generalizations about a population through point estimation. \n",
    "\n",
    "**Point Estimation** - is the use of sample data to measure a single value which serves as an approximate value or the best estimate of an unknown population parameter. \n",
    "\n",
    "There are a number of methods used for determining a best estimate including:\n",
    "1. **Method of Moments** - Estimates are found by equating the first k sample moments to the corresponding k population moments\n",
    "2. **Maximum of Likelihood** - Uses a model and the values in the model to maximize a likelihood function. This results in the most likely parameter for the inputs selected\n",
    "3. **Bayes' Estimators** - Minimizes the average risk (an expectation of random variables)\n",
    "4. **Best Unbiased Estimators** - Several unbiased estimators can be used to approximate a parameter (which one is 'best' depends on what paramter we are trying to find)\n",
    "\n",
    "## Interval Estimate\n",
    "By far the most used estimate method is **Interval Estimate** which uses an interval (or range of values) to estimate a population parameter (usually has lower and upper confidence limits)\n",
    "\n",
    "**Confidence Interval** is the measure of confidence that the interval estimate contains the population mean. Statisticians use the CI to describe the amount of uncertainty associated with a sample estimate of a population parameter\n",
    "\n",
    "### Confidence Interval Estimation Use Case\n",
    "\n",
    "<img src='data/images/stat1.png'>\n",
    "\n",
    "### Example\n",
    "<img src='data/images/stat2.png'>\n",
    "\n",
    "**Sampling Error** is the difference between the point estimate and the actual population parameter value \n",
    "\n",
    "**Margin Of Error** is the greatest possible distance between the point estimate and the value of the parameter it is estimating\n",
    "\n",
    "### Margin of Error Use Case\n",
    "Note that the 1.96 is the **Z-Score** taken from a z-score table where the mean is matched\n",
    "<img src='data/images/stat3.png'>\n",
    "\n",
    "\n",
    "## Hypothesis Testing\n",
    "Statisticians use hypothesis testing to formally check whether the hypothesis is accepted or rejected. Hypothesis testing is conducted in the following manner:\n",
    "1. **State the Hypothesis** - This stage involves stating the null and alternative hypothesis\n",
    "2. **Formulate an Analysis Plan** - This stage involves the construction of an anlysis plan\n",
    "3. **Analyse Sample Data** - This stage invloves the calculation and inerpretation of the test statistics as described in the anlaysis plan\n",
    "4. **Interpret Results** - This stage involves the application of the decision rule described in the analysis\n",
    "\n",
    "**Null Hypothesis** occurs when the result of a test is no different from the assumption\n",
    "\n",
    "**Alterante Hypothesis** occurs when the result disproves the assumption\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
